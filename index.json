[{"content":"Restic is a wonderfully simple backup program. I\u0026rsquo;ve used it on my Linux machines for a while. I recently decided to use it on macOS as well.\nrestic --repo /Volumes/restic-repo --verbose backup ~/Documents ~/Pictures If you run this on macOS, you might get an error like the following from restic. The error indicates that restic can\u0026rsquo;t open the Photos library folder.\ncan not obtain extended attribute com.apple.fileprovider.ignore#P for /Users/blachniet/Pictures/Photos Library.photoslibrary: can not obtain extended attribute com.apple.quarantine for /Users/blachniet/Pictures/Photos Library.photoslibrary: error: Open: open /Users/blachniet/Pictures/Photos Library.photoslibrary: operation not permitted Fortunately, there\u0026rsquo;s an easy fix. Restic can\u0026rsquo;t access the folder because of the default privacy settings on macOS. To fix this, give the terminal application (I use iTerm) permissions to access your photos.\nRestart the terminal, run the backup again and rest easy knowing your files are backed up 🎉!\n","permalink":"https://blachniet.github.io/posts/back-up-macos-photos-restic/","summary":"Restic is a wonderfully simple backup program. I\u0026rsquo;ve used it on my Linux machines for a while. I recently decided to use it on macOS as well.\nrestic --repo /Volumes/restic-repo --verbose backup ~/Documents ~/Pictures If you run this on macOS, you might get an error like the following from restic. The error indicates that restic can\u0026rsquo;t open the Photos library folder.\ncan not obtain extended attribute com.apple.fileprovider.ignore#P for /Users/blachniet/Pictures/Photos Library.photoslibrary: can not obtain extended attribute com.","title":"Back up macOS Photos with Restic"},{"content":"Use the manual plugin and DNS challenge in certbot to obtain a wildcard Let\u0026rsquo;s Encrypt TLS certificate.\n Subdomains only. You can only use this wildcard certificate on subdomains (e.g. www.example.com, mail.example.com). You cannot use it for the apex domain (e.g. example.com). Obtain a separate certificate for the apex domain.\n $ certbot certonly --manual --preferred-challenges dns -d \u0026#39;*.example.com\u0026#39; Create the TXT record as instructed by certbot. Before continuing, use dig\nor Google\u0026rsquo;s Dig tool to confirm the records is applied.\n$ dig txt _acme-challenge.example.com Wait until dig shows that the record is applied. You may want to refresh/re-run it a couple times to ensure the record is updated on a few different servers.\nOnce you\u0026rsquo;re confident the record update is applied, press Enter to continue the certbot process and continue following the instructions it provides.\n","permalink":"https://blachniet.github.io/posts/obtain-a-wildcard-lets-encrypt-certificate/","summary":"Use the manual plugin and DNS challenge in certbot to obtain a wildcard Let\u0026rsquo;s Encrypt TLS certificate.\n Subdomains only. You can only use this wildcard certificate on subdomains (e.g. www.example.com, mail.example.com). You cannot use it for the apex domain (e.g. example.com). Obtain a separate certificate for the apex domain.\n $ certbot certonly --manual --preferred-challenges dns -d \u0026#39;*.example.com\u0026#39; Create the TXT record as instructed by certbot. Before continuing, use dig","title":"Obtain a wildcard Let's Encrypt certificate"},{"content":"You can configure Fossil\u0026rsquo;s gdiff command to launch your favorite graphical diff tool. Below are examples configuring some popular diff tools.\nAll the examples below include the --global option. If you want to configure this setting only for the current repository, omit that option.\nBeyond Compare:\n$ fossil settings --global gdiff \u0026#39;bcomp\u0026#39; Git:\n$ fossil settings --global gdiff \u0026#39;code --diff --wait\u0026#39; vimdiff:\n$ fossil settings --global gdiff \u0026#39;vimdiff\u0026#39; Visual Studio Code:\n$ fossil settings --global gdiff \u0026#39;code --diff --wait\u0026#39; ","permalink":"https://blachniet.github.io/posts/use-a-graphical-diff-tool-with-fossil/","summary":"You can configure Fossil\u0026rsquo;s gdiff command to launch your favorite graphical diff tool. Below are examples configuring some popular diff tools.\nAll the examples below include the --global option. If you want to configure this setting only for the current repository, omit that option.\nBeyond Compare:\n$ fossil settings --global gdiff \u0026#39;bcomp\u0026#39; Git:\n$ fossil settings --global gdiff \u0026#39;code --diff --wait\u0026#39; vimdiff:\n$ fossil settings --global gdiff \u0026#39;vimdiff\u0026#39; Visual Studio Code:","title":"Use a graphical diff tool with Fossil"},{"content":"After changing your password on a remote Fossil repository, you will receive login failures when trying to sync your local repository.\n$ fossil sync Sync with https://blachniet@example.com/test Round-trips: 1 Artifacts sent: 0 received: 0 Error: login failed Round-trips: 1 Artifacts sent: 0 received: 0 Sync done, sent: 420 received: 285 ip: 192.168.0.10 $ fossil commit Autosync: https://blachniet@example.com/test Round-trips: 1 Artifacts sent: 0 received: 0 Error: login failed Round-trips: 1 Artifacts sent: 0 received: 0 Pull done, sent: 446 received: 287 ip: 192.168.0.10 Autosync failed. continue in spite of sync failure (y/N)? The error occurs because your local repository is still trying to use your old password to sync with the remote.\nTo update the password in your local repository, run fossil sync \u0026lt;url\u0026gt;. When provided with a URL, this command will prompt you for a password. Enter your new password and instruct Fossil to save it.\n$ fossil sync https://blachniet@example.com/test password for blachniet: remember password (Y/n)? Y Round-trips: 1 Artifacts sent: 0 received: 0 Sync done, sent: 456 received: 305 ip: 192.168.0.10 Future syncs will use the new password.\n","permalink":"https://blachniet.github.io/posts/resolve-fossil-login-failures-after-changing-your-password/","summary":"After changing your password on a remote Fossil repository, you will receive login failures when trying to sync your local repository.\n$ fossil sync Sync with https://blachniet@example.com/test Round-trips: 1 Artifacts sent: 0 received: 0 Error: login failed Round-trips: 1 Artifacts sent: 0 received: 0 Sync done, sent: 420 received: 285 ip: 192.168.0.10 $ fossil commit Autosync: https://blachniet@example.com/test Round-trips: 1 Artifacts sent: 0 received: 0 Error: login failed Round-trips: 1 Artifacts sent: 0 received: 0 Pull done, sent: 446 received: 287 ip: 192.","title":"Resolve Fossil login failures after changing your password"},{"content":"A leaf in Homebrew is a formula that is not a dependency of another installed formula. You can list the currently installed leaves using the brew leaves command. See thoughtbot\u0026rsquo;s brew leaves post for more information on this command.\nRemoving a leaf in Homebrew can suddenly introduce many more leaves; the dependencies of the formula you removed. In this post I will show you how to remove a leaf as well as all its dependencies not used by any other formulae. These steps will work in Bash and Zsh.\nCreate a text file containing regex patterns for all the leaves. We\u0026rsquo;ll use this file later as input to grep.\n$ brew leaves | xargs -I {} echo \u0026#39;^{}$\u0026#39; \u0026gt; leaves.txt Edit leaves.txt, removing any that you want to uninstall.\nGet the list of applications to remove, then remove them. Repeat this process until brew leaves | grep ... output is empty.\n$ brew leaves | grep -v --file leaves.txt gcc $ brew remove gcc Uninstalling /usr/local/Cellar/gcc/10.2.0_4... (1,465 files, 339.5MB) $ brew leaves | grep -v --file leaves.txt isl libmpc $ brew remove isl libmpc Uninstalling /usr/local/Cellar/isl/0.23... (72 files, 5MB) Uninstalling /usr/local/Cellar/libmpc/1.2.1... (13 files, 423.6KB) Alternatively, you can perform the list and remove in one step:\n$ brew leaves | grep -v --file leaves.txt | xargs brew remove Uninstalling /usr/local/Cellar/mpfr/4.1.0... (29 files, 5.1MB) ","permalink":"https://blachniet.github.io/posts/remove-leaf-and-dependencies-in-homebrew/","summary":"A leaf in Homebrew is a formula that is not a dependency of another installed formula. You can list the currently installed leaves using the brew leaves command. See thoughtbot\u0026rsquo;s brew leaves post for more information on this command.\nRemoving a leaf in Homebrew can suddenly introduce many more leaves; the dependencies of the formula you removed. In this post I will show you how to remove a leaf as well as all its dependencies not used by any other formulae.","title":"Remove leaf and dependencies in Homebrew"},{"content":"I\u0026rsquo;ve decided to shut down the Rager service at the end of October. I haven\u0026rsquo;t had time to improve or maintain the service lately. Some recent outages have gone unresolved for weeks!\nI suggest that those who were using this service consider Libraries.io as an alternative. Libraries.io has:\n Support for many project repositories and package managers Many features A team supporting the project Corporate sponsorship It\u0026rsquo;s open source!  ","permalink":"https://blachniet.github.io/posts/retiring-rager/","summary":"I\u0026rsquo;ve decided to shut down the Rager service at the end of October. I haven\u0026rsquo;t had time to improve or maintain the service lately. Some recent outages have gone unresolved for weeks!\nI suggest that those who were using this service consider Libraries.io as an alternative. Libraries.io has:\n Support for many project repositories and package managers Many features A team supporting the project Corporate sponsorship It\u0026rsquo;s open source!  ","title":"Retiring Rager"},{"content":"Google\u0026rsquo;s Cloud Platform documentation is top of the line. Even with the documentation though, it took me some time to figure out how to use the Cloud Vision API from a Go application running on a Compute Engine instance. All the right information is there, but it\u0026rsquo;s scattered. Below I\u0026rsquo;ve compiled this information into a few steps.\nAuthenticating with the Cloud Vision API involves more setup than the Cloud Datastore, Cloud Pub/Sub and Cloud Storage. The client libraries for the latter use Google Application Default Credentials and \u0026ldquo;just work\u0026rdquo; when running on a Compute Engine instance. In order to authenticate with the Cloud Vision API, you need to create a service account key and instruct the client library to use that key when sending API requests. See Authenticating to the Cloud Vision API for more details.\nPrerequisites  You have a Google Cloud project You have a Compute Engine instance and a service account associated with it (e.g. 123456789-compute@developer.gserviceaccount.com) You have a Go app that uses the Cloud Vision API client library  Steps   Enable the Cloud Vision API for your project: https://console.cloud.google.com/apis/api/vision.googleapis.com/overview\n  If you haven\u0026rsquo;t already, create a JSON key for the service account running the Compute Engine instance: https://console.cloud.google.com/iam-admin/serviceaccounts/project. Click the dropdown beside the service account, click Create Key and select the JSON key type.\n  Copy the key JSON file to your Compute Engine instance\n  When creating the Vision API client in Go, provide the service account file with option.WithServiceAccountFile(\u0026quot;/path/to/key.json\u0026quot;):\nvisionClient, err := vision.NewImageAnnotatorClient(ctx, option.WithServiceAccountFile(\u0026quot;/path/to/key.json\u0026quot;)) if err != nil { return nil, nil, errors.Wrap(err, \u0026quot;Error creating vision client\u0026quot;) }   ","permalink":"https://blachniet.github.io/posts/auth-cloud-vision-go/","summary":"Google\u0026rsquo;s Cloud Platform documentation is top of the line. Even with the documentation though, it took me some time to figure out how to use the Cloud Vision API from a Go application running on a Compute Engine instance. All the right information is there, but it\u0026rsquo;s scattered. Below I\u0026rsquo;ve compiled this information into a few steps.\nAuthenticating with the Cloud Vision API involves more setup than the Cloud Datastore, Cloud Pub/Sub and Cloud Storage.","title":"Authenticate Cloud Vision Client in Go"},{"content":"After setting up the systemd service for Caddy, I received the following error when trying to start the service:\n$ sudo systemctl start caddy.service Job for caddy.service failed. See \u0026#39;systemctl status caddy.service\u0026#39; and \u0026#39;journalctl -xn\u0026#39; for details. $ systemctl status caddy.service Failed to get D-Bus connection: No such file or directory The fix was pretty easy. It turns out that dbus was not installed on the system. I installed it and then the service worked perfectly:\n$ apt-get install -y dbus For what it\u0026rsquo;s worth, I did this on a Google Cloud Compute Engine instance that I initialized with the default Debian install.\n","permalink":"https://blachniet.github.io/posts/failed-to-get-d-bus-connection/","summary":"After setting up the systemd service for Caddy, I received the following error when trying to start the service:\n$ sudo systemctl start caddy.service Job for caddy.service failed. See \u0026#39;systemctl status caddy.service\u0026#39; and \u0026#39;journalctl -xn\u0026#39; for details. $ systemctl status caddy.service Failed to get D-Bus connection: No such file or directory The fix was pretty easy. It turns out that dbus was not installed on the system. I installed it and then the service worked perfectly:","title":"Failed to get D-Bus connection"},{"content":"Here\u0026rsquo;s a curated list of the articles I read this weekend.\nNegative feedback antipatterns Charles-Axel shares a template for providing negative feedback in a constructive manner. via Software Lead Weekly\nThe Books I Recommend For The New Manager I purchased Managing Oneself after reading this post. Google Play had it for less than $7. via Software Lead Weekly\nVisual Studio Development – Productivity Enhancements in Visual Studio 2017 RC Check out some of the productivity enhancements coming in the new Visual Studio. via MSDN Magazine\n","permalink":"https://blachniet.github.io/posts/weekend-reading-for-dec-04/","summary":"Here\u0026rsquo;s a curated list of the articles I read this weekend.\nNegative feedback antipatterns Charles-Axel shares a template for providing negative feedback in a constructive manner. via Software Lead Weekly\nThe Books I Recommend For The New Manager I purchased Managing Oneself after reading this post. Google Play had it for less than $7. via Software Lead Weekly\nVisual Studio Development – Productivity Enhancements in Visual Studio 2017 RC Check out some of the productivity enhancements coming in the new Visual Studio.","title":"Weekend Reading for Dec 04"},{"content":"Rager is service that notifies you when the projects you care about most publish a new release. After signing up using your GitHub account, you can start \u0026ldquo;watching\u0026rdquo; your favorite projects hosted by GitHub, NuGet or npm. Every night Rager scans for new releases. If it finds one that you are \u0026ldquo;watching\u0026rdquo;, it sends you an email with a link to the new release!\nThis application is a side project that I\u0026rsquo;ve been tinkering with for a while now. I had two goals for this project.\nFirst, I wanted to solve a simple problem that bothered me. I wanted to keep the libraries that I used in my professional and personal projects up-to-date. I hated that I had to manually go out and check the projects that I used for new releases. I found some alternative solutions that worked, but they had some quirks.\nThe first thing I tried was using GitHub\u0026rsquo;s watch feature. The biggest downside to this approach is that I got a notifications for any activity on the project. That means any new pull requests and issues. All I\u0026rsquo;m really interested in is the new releases, so my inbox was flooded with notifications I didn\u0026rsquo;t care about.\nMy second attempt was to subscribe to the atom feeds for releases. This was better, but still required me to exert the effort to actually check the feed for new updates.\nI wasn\u0026rsquo;t happy with either of these approaches, so I created Rager. With Rager, once I start \u0026ldquo;watching\u0026rdquo; the project, there\u0026rsquo;s no extra effort involved in finding the releases, and I only get notifications for updates that I care about.\nThe second reason I created Rager was to learn something new. I\u0026rsquo;m trying very hard to avoid becoming a dark matter developer. By day, I\u0026rsquo;m your standard enterprise C# developer, but by night I like to dabble in a little bit of everything. Every once in a while I learn something in my off-time that I get to apply to my professional work, which is one of the most rewarding parts of working on side-projects.\nI\u0026rsquo;m not sure that Rager provides a service that anyone else cares about, but it at least scratches my own itch.\nIf you want to keep libraries that you use up-to-date, then check it out. If you\u0026rsquo;ve got questions, check out the Rager FAQ or tweet @blachniet or @ragerhq. If you try it out, I\u0026rsquo;d love to hear what you think!\n","permalink":"https://blachniet.github.io/posts/introducing-rager/","summary":"Rager is service that notifies you when the projects you care about most publish a new release. After signing up using your GitHub account, you can start \u0026ldquo;watching\u0026rdquo; your favorite projects hosted by GitHub, NuGet or npm. Every night Rager scans for new releases. If it finds one that you are \u0026ldquo;watching\u0026rdquo;, it sends you an email with a link to the new release!\nThis application is a side project that I\u0026rsquo;ve been tinkering with for a while now.","title":"Introducing Rager"},{"content":"Constantly reevaluate the logging level applied to log events. Your typing was not directed by angels and your code was not ordained by God when you selected warning as the level for your log event. That code is up for change just like the rest.\nToo often I see piles of log events with poorly assigned log levels. If there’s an error event flooding your logs that you have no intention to fix, change the logging level. If life can move on normally with that log event, it’s not worthy of the error level.\nThe biggest problem with incorrectly assigned logging levels is that it can desensitize developers and operations to the importance of those events. When an error occurs, you want people to be surprised. You want them to investigate and resolve the issue quickly. If they have a constant flow of the same error events, the error level is going to lose its oomph.\nSo how do you choose a logging level? That requires some coordination with your team. The team needs to decide what logging levels are available, what they mean, and when they should be used.\nI don’t want a developer to have to think long and hard about the level for an event. I prefer to keep it as simple as possible. I do this by encouraging devs to feel free to change the logging level in the future and by providing and small set of common logging levels with clear distinctions. Here’s the levels I like to use the most:\n FATAL - Something\u0026rsquo;s very wrong. So wrong, in fact, that we had to drop everything and bail immediately. A human needs to investigate this as soon as possible. If the application is a one-off task that produces output, the output is likely incomplete, and should not be trusted. If the application is a service or interactive experience, it’s probably dead. ERROR - Again, there’s something very wrong and a human needs to investigate this as soon as possible. However, we were able to recover and continue. If the application is a one-off task producing output, you can probably glean some useful information from the output, but it is probably incomplete. WARN - There’s something strange going on. Maybe we’re seeing data that we’ve never seen before. Maybe the user did something strange. Maybe a new version of the web API we use is available. Whatever it was, our application is still in a good state and is still running fine. However, a human should investigate this soon and resolve the warning. INFO - This level is for everything else. Application flow, statistics, metrics, and other general information.  You might have noticed that I left off a few of the common logging levels such as VERBOSE, DEBUG and TRACE. With well structured log events and good analysis tools, events with that kind of information can be collapsed into the INFO level and still be easily filtered and distinguished. Again, a major goal with this level structure is to require as little thought and consideration as possible when choosing a level.\nI\u0026rsquo;m soooo nice!! photo by clement127 used under CC\n","permalink":"https://blachniet.github.io/posts/log-levels/","summary":"Constantly reevaluate the logging level applied to log events. Your typing was not directed by angels and your code was not ordained by God when you selected warning as the level for your log event. That code is up for change just like the rest.\nToo often I see piles of log events with poorly assigned log levels. If there’s an error event flooding your logs that you have no intention to fix, change the logging level.","title":"Reevaluate Your Log Levels"},{"content":"VagrantPress and VVV use the Vagrant::Hostsupdater plugin to allow you to access your development WordPress sites at a friendlier URL (e.g. http://vagrantpress.dev instead of http://localhost:8080). On Windows machines, the plugin updates the file at C:\\Windows\\System32\\drivers\\etc\\hosts, but modifying this files requires admin access. You must run your shell as an administrator for the plugin to be able to update this file when you vagrant up.\n\nBONUS: I\u0026rsquo;ve seen a lot of people installing Cygwin or Putty just to be able to use Vagrant on Windows (since it requires SSH). If you have Git Bash installed on your machine, which is included in the standard git installer for Windows, you can use that instead. I\u0026rsquo;ve always used Git Bash and haven\u0026rsquo;t run into any serious limitations while working with Vagrant.\n","permalink":"https://blachniet.github.io/posts/vagrant-hostsupdater-requires-admin/","summary":"VagrantPress and VVV use the Vagrant::Hostsupdater plugin to allow you to access your development WordPress sites at a friendlier URL (e.g. http://vagrantpress.dev instead of http://localhost:8080). On Windows machines, the plugin updates the file at C:\\Windows\\System32\\drivers\\etc\\hosts, but modifying this files requires admin access. You must run your shell as an administrator for the plugin to be able to update this file when you vagrant up.\n\nBONUS: I\u0026rsquo;ve seen a lot of people installing Cygwin or Putty just to be able to use Vagrant on Windows (since it requires SSH).","title":"Vagrant Hostsupdater Requires Admin"},{"content":"I\u0026rsquo;ve been working on a new web app lately and decided to allow users to sign up and log in with their GitHub account. I\u0026rsquo;m using go-github to interact with the GitHub API and with this library there are two different ways to get a user\u0026rsquo;s email address.\nIf you want to get the current user\u0026rsquo;s primary email address, use the Users.ListEmails() method. This method returns all the emails associated with the GitHub user and indicates whether the email address is their Primary address. In order to use this method, you need to request the user or user:email scope when acquiring the OAuth access token.\n// Print the current user's primary email address emails, _, _ := githubClient.Users.ListEmails(\u0026amp;github.ListOptions{PerPage:10}) for _, e := range emails{ if *e.Primary{ fmt.Printf(\u0026quot;Primary Email Address: %v\u0026quot;, *e.Email) break; } } If you are using GitHub for authentication, then the above method is what you are looking for. If you don\u0026rsquo;t actually require an email address and don\u0026rsquo;t want to request the user or user:email scope, you can retrieve the user\u0026rsquo;s publicly visible email address with Users.Get(\u0026quot;\u0026quot;). Keep in mind that this may be nil, as you are not required to make an email address publicly visible in GitHub.\n// Get the current user's public email address user, _, _ := githubClient.Users.Get(\u0026quot;\u0026quot;) if user.Email != nil{ fmt.Printf(\u0026quot;Public Email Address: %v\u0026quot;, *user.Email) } ","permalink":"https://blachniet.github.io/posts/go-github-user-email/","summary":"I\u0026rsquo;ve been working on a new web app lately and decided to allow users to sign up and log in with their GitHub account. I\u0026rsquo;m using go-github to interact with the GitHub API and with this library there are two different ways to get a user\u0026rsquo;s email address.\nIf you want to get the current user\u0026rsquo;s primary email address, use the Users.ListEmails() method. This method returns all the emails associated with the GitHub user and indicates whether the email address is their Primary address.","title":"User Email Addresses with Go-GitHub"},{"content":"The System.Diagnostics.Process class provides the options to retrieve all output from an external process after it has completed, or to retrieve events containing the output as the execution occurs. The option to receive asynchronous output events is useful in scenarios where you need to provide user feedback or log the output when timeouts occur.\nWhen receiving asynchronous output events, you must ensure that the external process has completed when you want to make sure that you get all the output. However, waiting for HasExited to be true is not enough. In order to ensure that all processing has completed, including the handling of asynchronous output events, you must call Process.WaitForExit(). You need to call the overload taking no arguments, even if you previously called Process.WaitForExit(Int32) overload. The Process.WaitForExit(Int32) docs state:\n To ensure that asynchronous event handling has been completed, call the WaitForExit() overload that takes no parameter after receiving a true from this overload.\n Below is a simple example that uses Process.WaitForExit(Int32) to enforce timeouts and Process.WaitForExit() to flush the output events when the process completes.\nstatic void DoExternalThing() { var proc = new Process { EnableRaisingEvents = true, StartInfo = new ProcessStartInfo { FileName = \u0026#34;foo.exe\u0026#34;, UseShellExecute = false, RedirectStandardError = true, RedirectStandardOutput = true, }, }; proc.ErrorDataReceived += proc_ErrorDataReceived; proc.OutputDataReceived += proc_OutputDataReceived; proc.Start(); proc.BeginErrorReadLine(); proc.BeginOutputReadLine(); // Timeout after 10 seconds  if (!proc.WaitForExit(10000)) { Console.WriteLine(\u0026#34;Operation timed out\u0026#34;); return; } // Flush async events  proc.WaitForExit(); Console.WriteLine(\u0026#34;Operation complete.\u0026#34;); } ","permalink":"https://blachniet.github.io/posts/flush-async-output-events/","summary":"The System.Diagnostics.Process class provides the options to retrieve all output from an external process after it has completed, or to retrieve events containing the output as the execution occurs. The option to receive asynchronous output events is useful in scenarios where you need to provide user feedback or log the output when timeouts occur.\nWhen receiving asynchronous output events, you must ensure that the external process has completed when you want to make sure that you get all the output.","title":"Flush Async Output Events"},{"content":"Last month I gave a presentation on using Serilog, Elasticsearch and Kibana for logging from an application. You can see the slides from that presentation below. I\u0026rsquo;ve had a lot of success with this trio lately. It has provided a lot of details about what is going on in my application that would have been more difficult to obtain before.\n ","permalink":"https://blachniet.github.io/posts/kens-structured-logging-slides/","summary":"Last month I gave a presentation on using Serilog, Elasticsearch and Kibana for logging from an application. You can see the slides from that presentation below. I\u0026rsquo;ve had a lot of success with this trio lately. It has provided a lot of details about what is going on in my application that would have been more difficult to obtain before.\n ","title":"KENS Structured Logging Slides"},{"content":"Below is a collection of good habits I\u0026rsquo;ve identified in my recent experiences with structured logging. The code examples are specific to Serilog, but the ideas can be applied to other structured logging tool sets (my stack of choice for .NET projects is Serilog, Elasticsearch and Kibana). Got your own structured logging practices to contribute? Share them on Twitter!\nEventID When you create you log events, include an EventID property to uniquely identify the event. This will make it easier to build queries when analyzing your logs later.\nI like to make my event ids stand out in the message, so I generally wrap them in \u0026lt;\u0026gt; or something similar. I also include the :l modifier to leave out the double quotes in the message.\nlog.Information( \u0026quot;\u0026lt;{EventID:l}\u0026gt; {Handler} - {ElapsedMilliseconds}ms\u0026quot;, \u0026quot;HandlerCompleted\u0026quot;, handler.GetType().Name, stopwatch.ElapsedMilliseconds); /// Message: /// \u0026lt;HandlerCompleted\u0026gt; \u0026quot;IndexHandler\u0026quot; - 250ms SourceContext Always include the code entity responsible for creating the log event. This will help you narrow down where problems are coming from, particularly if you have multiple code entities that produce the same EventID. Serilog provides an easy way to do this with the ForContext\u0026lt;T\u0026gt;() method on loggers which adds a SourceContext property on the log event.\nvar log = baseLogger.ForContext\u0026lt;IndexHandler\u0026gt;(); log.Error(\u0026quot;\u0026lt;{EventID:l}\u0026gt;\u0026quot;, \u0026quot;DbConnErr\u0026quot;); // JSON: // { // \u0026quot;EventID\u0026quot;: \u0026quot;DbConnErr\u0026quot;, // \u0026quot;SourceContext\u0026quot;: \u0026quot;IndexHandler\u0026quot; // } ContextID If you have a stream of log events that are related (e.g. multiple log events from a single HTTP transaction), include a property to link them all together. I generally call mine ContextID. When you are analyzing your logs later and want to look at a single transaction, you can filter by this property. You can use the ForContext() method on loggers to attach this property to log events without including it in the log message.\nlog.ForContext(\u0026quot;ContextID\u0026quot;, transactionID) .Information(\u0026quot;\u0026lt;{EventID:l}\u0026gt;\u0026quot;, \u0026quot;TransactionStarted\u0026quot;); // JSON: // { // \u0026quot;EventID\u0026quot;: \u0026quot;TransactionStarted\u0026quot;, // \u0026quot;SourceContext\u0026quot;: \u0026quot;IndexHandler\u0026quot;, // \u0026quot;ContextID\u0026quot;: \u0026quot;e6f82c53-557f-448d-ab75-f05e344140bc\u0026quot; // } Terse Messages Try to keep your log messages brief. A good log event message gives a very short summary of a log event, not extended details about the event. Most of my log event messages only include an EventID and one or two scalar properties.\nI\u0026rsquo;m not saying to exclude details from log events, just don\u0026rsquo;t include them in the message. For example, don\u0026rsquo;t destructure big objects in your messages. It just makes the message hard to read. Your log events are ideally going to a destination that can handle structured events, like Elasticsearch, so you can still query on those details even when they are not included in the message.\nvar user = new User{ Name = \u0026quot;John\u0026quot;, Email = \u0026quot;john.doe@example.com\u0026quot;, Password = \u0026quot;p1nkBunni3s\u0026quot; }; // Do This // Message: \u0026lt;NewUser\u0026gt; \u0026quot;john.doe@example.com\u0026quot; log.ForContext(\u0026quot;User\u0026quot;, user, destructureObjects=true) .Information(\u0026quot;\u0026lt;{EventID:l}\u0026gt; {Email}\u0026quot;, \u0026quot;NewUser\u0026quot;, user.Email); // Don't Do This // Message: \u0026lt;NewUser\u0026gt; { \u0026quot;User\u0026quot;: {\u0026quot;Name\u0026quot;: \u0026quot;John\u0026quot;, \u0026quot;Email\u0026quot;: \u0026quot;john.doe@example.com\u0026quot;, \u0026quot;Password\u0026quot;: \u0026quot;p1nkBunni3s\u0026quot;}} log.Information(\u0026quot;\u0026lt;{EventID:l}\u0026gt; {User}\u0026quot;, \u0026quot;NewUser\u0026quot;, user); Extensions Add some extension methods to make logging even easier. For example, I added extension methods that allowed me to include the EventID in the log message without having to manually add it to the message template for each event.\npublic static void InformationEvent(this ILogger logger, string eventID, string messageTemplate, params object[] propertyValues) { var allProps = new object[]{ eventID }.Concat(propertyValues).ToArray(); logger.Information(\u0026quot;\u0026lt;{EventID:l}\u0026gt; \u0026quot; + messageTemplate, props); } // Example Usage: log.InformationEvent(\u0026quot;NewUser\u0026quot;, \u0026quot;{Email}\u0026quot;, user.Email); I also like to create an extension method to wrap calls to ForContext. I like the With terminology used by logrus, so I generally create an extension method with the same name which destructures the given value by default (ForContext does not destructure by default).\npublic static ILogger With(this ILogger logger, string propertyName, object value) { return logger.ForContext(propertyName, value, destructureObjects=true); } // Example Usage: log.With(\u0026quot;User\u0026quot;, user) .InformationEvent(\u0026quot;NewUser\u0026quot;, \u0026quot;{Email}\u0026quot;, user.email) Dispose Sinks If you are using a sink that derives from PeriodicBatchingSink, then you may want to ensure that you Dispose of your sink explicitly. Disposing the sink is the only way I\u0026rsquo;ve found to explicitly flush these sinks. If the sink has events in it that have not been sent when the application exits, those events will be lost if you don\u0026rsquo;t Dispose of it.\n","permalink":"https://blachniet.github.io/posts/serilog-good-habits/","summary":"Below is a collection of good habits I\u0026rsquo;ve identified in my recent experiences with structured logging. The code examples are specific to Serilog, but the ideas can be applied to other structured logging tool sets (my stack of choice for .NET projects is Serilog, Elasticsearch and Kibana). Got your own structured logging practices to contribute? Share them on Twitter!\nEventID When you create you log events, include an EventID property to uniquely identify the event.","title":"Good Habits for Structured Logging with Serilog"},{"content":"ThoughtWorks recently released their Technology Radar report for January 2015 and structured logging was among the techniques that they strongly suggested that the industry adopt. I\u0026rsquo;ve actually had some exposure to structured logging over the past year and have been very happy with the results. So, I thought I would share my experience.\nMy first exposure to structured logging was in early 2014. I was creating a simple Python application to monitor the amount traffic through my modem throughout the day and I was logging events from my application to logentries. I had just learned that logentries could extract key-value pairs from log events so I started formatting my log events to allow that (e.g. event=trafficUpdate megabytes=64.32). I then configured an alert to send me an email whenever the megabytes exceeded some value. I was really happy with this introduction to structured logging and, naturally, I wanted more.\n[caption id=\u0026ldquo;attachment_997\u0026rdquo; align=\u0026ldquo;aligncenter\u0026rdquo; width=\u0026ldquo;660\u0026rdquo;] Logentries is a cloud logging service that can extract key-value pairs from log events.[/caption]\nI wanted to bring what I had learned to work and apply it to our products, but we generally work in an offline environment, so using logentries was not an option. I was also tired of having to work so hard to manually format the log events to ensure that they could be parsed by the logging system. Serilog solved the formatting problem for me and the ELK stack provided the solution for storing and viewing log events in an offline network.\nSerilog is a logging library for .NET that makes it easy to generate and deliver structured logs. It ships with many different sinks - plugins to deliver log events to different providers - one of which is the Elasticsearch sink. Elasitcsearch (ELK) is a distributed NoSQL datastore that provides tools that make it a great destination for log events. Logstash (ELK) is a service for collecting, parsing and storing logevents from different sources into Elasticsearch databases, and Kibana (ELK) is a web application for visualizing the log event data. Serilog is actually capable of delivering log events directly to Elasticsearch in the Logstash format, so you can skip Logstash altogether if you are only ingesting events from Serilog.\n[caption id=\u0026ldquo;attachment_1000\u0026rdquo; align=\u0026ldquo;aligncenter\u0026rdquo; width=\u0026ldquo;660\u0026rdquo;] This screenshot shows Kibana being used in the Packetbeat demo.[/caption]\nI used this trio (Serilog, Elasticsearch and Kibana) in one of our new products and I have been very happy with the results. With well structured log events, we are able to extract metrics from and gain insights into our application that we had to build separate systems to support in the past. Previously, when we wanted to be able to obtain some metrics from our applications, we would create an application to import logs into a database specialized for storing those metrics and a front-end application for viewing those metrics. While there\u0026rsquo;s certainly still room for that sort of specialized setup, our new structured logging approach made it really easy to get started extracting this data with very little effort. If we decide that we want to extract some new metric, all we have to do is add a log event to the application to communicate that metric and configure a Kibana widget to display the data in a pretty graph. No code changes required to a log importer, database schema, or front-end viewer.\nModel of graphene structure by CORE-Materials used under Creative Commons.\n","permalink":"https://blachniet.github.io/posts/structured-logging-serilog-elk/","summary":"ThoughtWorks recently released their Technology Radar report for January 2015 and structured logging was among the techniques that they strongly suggested that the industry adopt. I\u0026rsquo;ve actually had some exposure to structured logging over the past year and have been very happy with the results. So, I thought I would share my experience.\nMy first exposure to structured logging was in early 2014. I was creating a simple Python application to monitor the amount traffic through my modem throughout the day and I was logging events from my application to logentries.","title":"Structured Logging with Serilog and ELK"},{"content":"I wasted away a good thirty minutes trying to figure out why I couldn\u0026rsquo;t parse my YAML config file this morning using go-yaml.\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;gopkg.in/yaml.v2\u0026quot; ) var configText = ` Message: Welcome UpdateInterval: 5 EmailAddresses: - john.doe@example.com - jane.doe@example.com` type Config struct { Message string UpdateInterval int EmailAddresses []string } func main() { var c Config if err := yaml.Unmarshal([]byte(configText), \u0026amp;c); err != nil { log.Fatalf(\u0026quot;Failed to parse yaml: %v\u0026quot;, err) } fmt.Printf(\u0026quot;Config: %+v\\n\u0026quot;, c) } Output:\nConfig: {Message: UpdateInterval:0 EmailAddresses:[]} As you can see in the output, no errors occur but the struct fields are not initialized properly. Message should be Welcome, UpdateInterval should be 5, and there should be two email addresses.\nIt turns out that go-yaml expects the YAML field corresponding to a struct field to be lowercase. So if your struct field is UpdateInterval, the corresponding field in YAML is updateinterval. When I changed my config, I got the output I was expecting:\nvar configText = ` message: Welcome updateinterval: 5 emailaddresses: - john.doe@example.com - jane.doe@example.com` Output:\nConfig: {Message:Welcome UpdateInterval:5 EmailAddresses:[john.doe@example.com jane.doe@example.com]} I didn\u0026rsquo;t really like the all-lowercase field names in my config, so I used struct field tags to tell go-yaml what the corresponding YAML field names will look like and reverted to my original config:\nvar configText = ` Message: Welcome UpdateInterval: 5 EmailAddresses: - john.doe@example.com - jane.doe@example.com` type Config struct { Message string `yaml:\u0026quot;Message\u0026quot;` UpdateInterval int `yaml:\u0026quot;UpdateInterval\u0026quot;` EmailAddresses []string `yaml:\u0026quot;EmailAddresses\u0026quot;` } Output:\nConfig: {Message:Welcome UpdateInterval:5 EmailAddresses:[john.doe@example.com jane.doe@example.com]} Hopefully this will save someone else some time, because this drove me crazy this morning.\n","permalink":"https://blachniet.github.io/posts/go-yaml-field-names/","summary":"I wasted away a good thirty minutes trying to figure out why I couldn\u0026rsquo;t parse my YAML config file this morning using go-yaml.\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;gopkg.in/yaml.v2\u0026quot; ) var configText = ` Message: Welcome UpdateInterval: 5 EmailAddresses: - john.doe@example.com - jane.doe@example.com` type Config struct { Message string UpdateInterval int EmailAddresses []string } func main() { var c Config if err := yaml.Unmarshal([]byte(configText), \u0026amp;c); err != nil { log.","title":"Go-YAML Field Names"},{"content":"I learned an important lesson today: do not assert inside of Moq callbacks. In the code below, even if the Assert.AreEqual fails, it will not cause the unit test to fail.\nmoqObj.Setup(m =\u0026gt; m.Doit(It.IsAny\u0026lt;string\u0026gt;())).Callback\u0026lt;string\u0026gt;((s) =\u0026gt; { Assert.AreEqual(\u0026quot;foo\u0026quot;, s); }); // Insert test code that invokes `Doit` You could use Verify instead, but it\u0026rsquo;s not always ideal, particularly when trying to step through the assertions while debugging tests. Instead, you can take this approach suggested by Thomas Ardal.\nstring actual = null; moqObj.Setup(m =\u0026gt; m.Doit(It.IsAny\u0026lt;string\u0026gt;())).Callback\u0026lt;string\u0026gt;((s) =\u0026gt; { actual = s; }); // Insert test code that invokes `Doit` Assert.AreEqual(\u0026quot;foo\u0026quot;, actual); ","permalink":"https://blachniet.github.io/posts/assertions-in-moq-callbacks/","summary":"I learned an important lesson today: do not assert inside of Moq callbacks. In the code below, even if the Assert.AreEqual fails, it will not cause the unit test to fail.\nmoqObj.Setup(m =\u0026gt; m.Doit(It.IsAny\u0026lt;string\u0026gt;())).Callback\u0026lt;string\u0026gt;((s) =\u0026gt; { Assert.AreEqual(\u0026quot;foo\u0026quot;, s); }); // Insert test code that invokes `Doit` You could use Verify instead, but it\u0026rsquo;s not always ideal, particularly when trying to step through the assertions while debugging tests. Instead, you can take this approach suggested by Thomas Ardal.","title":"Assertions in Moq Callbacks"},{"content":"I have wanted to revamp my website for close to a year now. In that time I\u0026rsquo;ve looked into different blogging engines and tried creating new themes, but I didn\u0026rsquo;t settle on a path forward until a couple weeks ago.\nEngine At one point I considered moving to a statically generated site since they were all the rage. I tried out Jekyll and Middleman. I liked both and even wrote up a post on using Zurb\u0026rsquo;s Foundation with Middleman. However, I decided that by sticking with WordPress I could focus more on creating content.\nMarkdown Something that I really like about the static site generators is their first-class support for Markdown. I routinely write Markdown, so I felt that if I could generate my posts in Markdown, I would have one less hurdle for generating content. Before, I was using the WP-Markdown plugin, but I don\u0026rsquo;t like that it doesn\u0026rsquo;t retain the original Markdown content. Instead, it generates and saves the HTML when the post is saved, then converts it back to Markdown when editing the post. This probably works for most people, but I was having trouble with some of my code blocks getting messed up during the conversions. I decided to start using the Markdown feature in the Jetpack plugin. This plugin retains the original Markdown, and I\u0026rsquo;ve been very happy with it so far. This plugin also works well highlight.js, which I\u0026rsquo;m using for syntax highlight on the site.\nPlanning The biggest problem I have had so far with maintaining my blog is generating posts. Over 3 years I have only published 31 posts, and only 2 in 2014. So, I decided to be more deliberate about pulling together post ideas and notes. I created a Trello board where I\u0026rsquo;m maintaining a list of post ideas and collecting notes for posts that I\u0026rsquo;m working on. I use other Trello boards for other purposes and have had a lot of success with them, so I\u0026rsquo;m hopeful that this will improve my content creation process.\nDesign I really liked the new Twenty Fifteen theme for WordPress, which is why I decided to base my new theme on it. I created the Block 15 child theme to add some customizations for my site. It has a lot of stuff hardcoded specifically for my website, but I decided to open source it on GitHub in case it could help anyone else interested in creating their own child theme.\nDiscussion I decided to drop my use of Disqus with the new theme. The Disqus thread always looked so ugly on the site and significantly increased the page load time. Instead, I added a Twitter link at the bottom of each post, encouraging readers to start a conversation about the post on Twitter. In addition to avoiding an ugly, long discussion thread on posts, this also increases my site\u0026rsquo;s overall visibility. Some users that have never been to my blachniet.com might observe a conversation on Twitter, and decide to check out my site as a result.\nSo, here\u0026rsquo;s to better year of blogging in 2015.\n","permalink":"https://blachniet.github.io/posts/a-new-look-for-2015/","summary":"I have wanted to revamp my website for close to a year now. In that time I\u0026rsquo;ve looked into different blogging engines and tried creating new themes, but I didn\u0026rsquo;t settle on a path forward until a couple weeks ago.\nEngine At one point I considered moving to a statically generated site since they were all the rage. I tried out Jekyll and Middleman. I liked both and even wrote up a post on using Zurb\u0026rsquo;s Foundation with Middleman.","title":"A New Look for 2015"},{"content":"If you\u0026rsquo;re running a small website and using Google Analytics, your site speed metrics graph might look empty. If you don\u0026rsquo;t get a lot of traffic there will be no samples to measure site performance. This is because the default sampling rate is only 1% (i.e. site speed metrics will only be collected for 1% of your visitors). This is plenty if your running a big site with tons of visitors per day, but if your running a smaller site this isn\u0026rsquo;t going to be good enough.\nLuckily, Google Analytics allows you to change this setting. If you\u0026rsquo;re using Universal Analytics you can set the site speed sample rate when creating your tracker. In the example below I have set the tracker to sample 50% of my visitors by passing {'siteSpeedSampleRate': 50} in the create call.\n\u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt; (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-XXXX-Y', {'siteSpeedSampleRate': 50}); ga('send', 'pageview'); \u0026lt;/script\u0026gt; After doing this I got a significant increase in the number of samples per day, allowing me to get a better idea of how my site was performing.\n","permalink":"https://blachniet.github.io/posts/google-analytics-site-speed/","summary":"If you\u0026rsquo;re running a small website and using Google Analytics, your site speed metrics graph might look empty. If you don\u0026rsquo;t get a lot of traffic there will be no samples to measure site performance. This is because the default sampling rate is only 1% (i.e. site speed metrics will only be collected for 1% of your visitors). This is plenty if your running a big site with tons of visitors per day, but if your running a smaller site this isn\u0026rsquo;t going to be good enough.","title":"Google Analytics Site Speed for Small Websites"},{"content":"This weekend I decided to try out the static site generator, Middleman. While I was working on converting an existing static site, I decided to take the opportunity to try out Zurb\u0026rsquo;s Foundation and SCSS too. Andrea Moretti created a Middleman template to get you started doing just this, but I hate just using templates without understanding what they are doing. So I decided to start with Middleman\u0026rsquo;s HTML5 Boilerplate template and add the Foundation Bower package to it. You can check out the final product over on GitHub at https://github.com/blachniet/middleman-foundation. If you\u0026rsquo;re not interested in the inner workings, grab Andrea Morretti\u0026rsquo;s Middleman Foundation template.\nMiddleman Template and Bower Setup Initialize the project with the HTML5 Boilerplate.\n\u0026gt; middleman init middleman-foundation --template=html5 \u0026gt; cd middleman-foundation Create a .bowerrc file to specify a the install location for bower components\n// .bowerrc { \u0026quot;directory\u0026quot; : \u0026quot;source/bower_components\u0026quot; } Initialize bower for the project.\n\u0026gt; bower init Install Foundation.\n\u0026gt; bower install foundation --save Add the Compass configuration to config.rb.\n# config.rb compass_config do |config| # Require any additional compass plugins here. config.add_import_path \u0026quot;bower_components/foundation/scss\u0026quot; config.output_style = :compact end Add the bower directory to the Sprockets asset path in the config.rb.\n# config.rb # Add bower's directory to sprockets asset path after_configuration do @bower_config = JSON.parse(IO.read(\u0026quot;#{root}/.bowerrc\u0026quot;)) sprockets.append_path File.join \u0026quot;#{root}\u0026quot;, @bower_config[\u0026quot;directory\u0026quot;] end Replace CSS with SCSS Delete source/css/main.css and source/css/normalize.css. In their place add a new file, all.css.scss.\n# all.css.scss @import \u0026quot;normalize.scss\u0026quot;; @import \u0026quot;foundation.scss\u0026quot;; Replace the stylesheet references in the \u0026lt;head\u0026gt; of source/layouts/layout.erb with the all.css.scss.\n\u0026lt;!-- source/layouts/layouts.erb --\u0026gt; \u0026lt;%= stylesheet_link_tag \u0026quot;all\u0026quot; %\u0026gt; Use Modernizr from Bower Create a source/js/modernizr.js with the following content to include the modernizr source from the installed bower components.\n// source/js/modernizr.js //= require modernizr/modernizr Then replace the modernizr reference in layouts.erb\u0026rsquo;s head with \u0026lt;%= javascript_include_tag \u0026quot;modernizr\u0026quot; %\u0026gt;.\nUse Other Javascripts from Bower Delete all files (except the new modernizr.js) in source/js/, and create all.js with the following contents to include necessary javascripts from the bower components.\n// source/js/all.js //= require jquery/dist/jquery //= require foundation/js/foundation.min Then, update layouts.erb to reference your new all.js and remove the old javascript references.\n\u0026lt;!-- source/layouts/layout.erb --\u0026gt; \u0026lt;%= javascript_include_tag \u0026quot;all\u0026quot; %\u0026gt; Serve You\u0026rsquo;re all ready to go. Launch middleman server and start building!\n","permalink":"https://blachniet.github.io/posts/middleman-foundation/","summary":"This weekend I decided to try out the static site generator, Middleman. While I was working on converting an existing static site, I decided to take the opportunity to try out Zurb\u0026rsquo;s Foundation and SCSS too. Andrea Moretti created a Middleman template to get you started doing just this, but I hate just using templates without understanding what they are doing. So I decided to start with Middleman\u0026rsquo;s HTML5 Boilerplate template and add the Foundation Bower package to it.","title":"Middleman Foundation"},{"content":"UPDATE : This article provides a manual approach to creating a development copy of your website. However, I ran across this WPBeginner article which describes a much easier approach using the Duplicator plugin. I would suggest following that guide unless you prefer the manual process.\nBefore We Start Here\u0026rsquo;s my environment setup. This post should still be helpful if your setup isn\u0026rsquo;t exactly the same, though.\n GoDaddy Hosting (production) WAMP (development) phpMyAdmin (production + development)  Files Download WordPress Files Instead of using the WordPress export feature, we\u0026rsquo;re going to grab all site\u0026rsquo;s files. I\u0026rsquo;m going to use GoDaddy\u0026rsquo;s File Manager to get the files because it can archive all the files in a single zip which I can then download. If you are not using GoDaddy and your host doesn\u0026rsquo;t offer something similar then you can pull all the files over FTP using FileZilla.\nThe screenshots below show how I archived my files for download.\n[caption id=\u0026ldquo;attachment_831\u0026rdquo; align=\u0026ldquo;aligncenter\u0026rdquo; width=\u0026ldquo;300\u0026rdquo;][]2 Open the GoDaddy File Manager.[/caption][caption id=\u0026ldquo;attachment_832\u0026rdquo; align=\u0026ldquo;aligncenter\u0026rdquo; width=\u0026ldquo;300\u0026rdquo;] Select all the files you wish to download.[/caption][caption id=\u0026ldquo;attachment_829\u0026rdquo; align=\u0026ldquo;aligncenter\u0026rdquo; width=\u0026ldquo;300\u0026rdquo;] In the More dropdown select Archive.[/caption][caption id=\u0026ldquo;attachment_830\u0026rdquo; align=\u0026ldquo;aligncenter\u0026rdquo; width=\u0026ldquo;300\u0026rdquo;] Insert a name for the archive and download it.[/caption]\nExtract and Configure  Extract the files from the archive to your site directory (e.g. C:\\wamp\\www\\site_name). Update the wp-config.php DB_HOST (probably to localhost) Update other DB login info if it will be different than your production instance. If you are using non-default Permalink settings, ensure that the rewrite_module is enabled on your Apache instance.  Database I\u0026rsquo;m going to assume that you are familiar administrating with phpMyAdmin and MySQL.\nExport the Production Database  Login to your production phpMyAdmin instance. Select your database Select the export tab. Export you database to a SQL file.  Set Up the Development Database  Login to your development phpMyAdmin instance. Create a new user + database, preferably with the same name as the production instance. Make sure the host for the user is correct. It probably needs to be localhost.  Import Data Open the MySQL command prompt to your database.\n\u0026gt; mysql --user=user_name --password=P@assw0rd database_name In the mysql database prompt, execute the SQL file you exported.\nmysql\u0026gt; source database_name.sql Change your site\u0026rsquo;s location in the database with the following SQL. You will need to replace http://my.web.zz/dev with the URL to your developer instance.\nSELECT * FROM wp_options WHERE option_name = \u0026quot;home\u0026quot; OR option_name = \u0026quot;siteurl\u0026quot;; UPDATE wp_options SET option_value = \u0026quot;http://my.web.zz/dev\u0026quot; WHERE option_name = \u0026quot;home\u0026quot; OR option_name = \u0026quot;siteurl\u0026quot; Wrap-Up That should be everything you need to do. If your WAMP instance is running, you should be able to browse to your site. If you have any problems or suggestions, leave them in the comments below.\nReferences  Running a Development Copy of Wordpress | WordPress Codex  ","permalink":"https://blachniet.github.io/posts/create-development-version-existing-wordpress-site/","summary":"UPDATE : This article provides a manual approach to creating a development copy of your website. However, I ran across this WPBeginner article which describes a much easier approach using the Duplicator plugin. I would suggest following that guide unless you prefer the manual process.\nBefore We Start Here\u0026rsquo;s my environment setup. This post should still be helpful if your setup isn\u0026rsquo;t exactly the same, though.\n GoDaddy Hosting (production) WAMP (development) phpMyAdmin (production + development)  Files Download WordPress Files Instead of using the WordPress export feature, we\u0026rsquo;re going to grab all site\u0026rsquo;s files.","title":"Create a Development Version of an Existing WordPress Site"},{"content":"Having trouble deciding whether to use a div, section or article in your HTML5 page? Oli Studholme has a great article explaining how they should be used and provides a very simple method for deciding which to use.\n To decide which of these three elements is appropriate, choose the first suitable option:\n1. Would the enclosed content would make sense on it’s own in a feed reader? If so use \n2. Is the enclosed content related? If so use \n3. Finally if there’s no semantic relationship use \n Read the full article at http://bit.ly/1cVXQ7R.\n","permalink":"https://blachniet.github.io/posts/html5-articles-sections/","summary":"Having trouble deciding whether to use a div, section or article in your HTML5 page? Oli Studholme has a great article explaining how they should be used and provides a very simple method for deciding which to use.\n To decide which of these three elements is appropriate, choose the first suitable option:\n1. Would the enclosed content would make sense on it’s own in a feed reader? If so use","title":"HTML5 Articles and Sections"},{"content":"Use the following code in the BundleConfig.cs of your ASP.NET MVC application to force bundling and minification.\n// BundleConfig::RegisterBundles BundleTable.EnableOptimizations = true; Why? Why would you do this? Sometimes bundling and minification causes problems, such as in AngularJS. You will want to find these problems before publishing, so just enable this line every once in a while to make sure everything still works as expected with the optimizations.\n","permalink":"https://blachniet.github.io/posts/force-bundling-optimizations/","summary":"Use the following code in the BundleConfig.cs of your ASP.NET MVC application to force bundling and minification.\n// BundleConfig::RegisterBundles BundleTable.EnableOptimizations = true; Why? Why would you do this? Sometimes bundling and minification causes problems, such as in AngularJS. You will want to find these problems before publishing, so just enable this line every once in a while to make sure everything still works as expected with the optimizations.","title":"Force Bundling Optimizations"},{"content":"This post is a summary of this StackOverflow answer by Cheeso.\nTo read a file that is currently open (or may be open later) in another process, you must specify a FileShare option. The FileShare flag indicates the modes that other processes are allowed to open the file in.\nusing (Stream s = System.IO.File.Open(fullFilePath, FileMode.Open, FileAccess.Read, // I want to open this file for reading only. FileShare.ReadWrite)) // Other processes may specify // FileAccess of read or write. { } Reference  StackOverflow Answer FileShare Enumeration (MSDN) CreateFile function (MSDN)  ","permalink":"https://blachniet.github.io/posts/read-open-files-c/","summary":"This post is a summary of this StackOverflow answer by Cheeso.\nTo read a file that is currently open (or may be open later) in another process, you must specify a FileShare option. The FileShare flag indicates the modes that other processes are allowed to open the file in.\nusing (Stream s = System.IO.File.Open(fullFilePath, FileMode.Open, FileAccess.Read, // I want to open this file for reading only. FileShare.ReadWrite)) // Other processes may specify // FileAccess of read or write.","title":"Read Open File in C#"},{"content":"It\u0026rsquo;s common for web applications to use email addresses instead of user names to distinguish users. However, if you are using ASP.NET Identity, you have probably noticed that it has UserName built into the IUser interface. Since Identity assumes that this is the distinguishing field for the user, it\u0026rsquo;s not crazy to think that it might be a good place to drop the email address. In order to have Identity allow an email address in this field, you will need to write a custom IIdentityValidator.\n/// \u0026lt;summary\u0026gt; /// A replacement for the \u0026lt;see cref=\u0026quot;UserValidator\u0026quot;/\u0026gt; which requires that an email /// address be used for the \u0026lt;see cref=\u0026quot;IUser.UserName\u0026quot;/\u0026gt; field. /// \u0026lt;/summary\u0026gt; /// \u0026lt;typeparam name=\u0026quot;TUser\u0026quot;\u0026gt;Must be a type derived from \u0026lt;see cref=\u0026quot;Microsoft.AspNet.Identity.IUser\u0026quot;/\u0026gt;.\u0026lt;/typeparam\u0026gt; /// \u0026lt;remarks\u0026gt; /// This validator check the \u0026lt;see cref=\u0026quot;IUser.UserName\u0026quot;/\u0026gt; property against the simple email regex provided at /// http://www.regular-expressions.info/email.html. If a \u0026lt;see cref=\u0026quot;UserManager\u0026quot;/\u0026gt; is provided in the constructor, /// it will also ensure that the email address is not already being used by another account in the manager. /// /// To use this validator, just set \u0026lt;see cref=\u0026quot;UserManager.UserValidator\u0026quot;/\u0026gt; to a new instance of this class. /// \u0026lt;/remarks\u0026gt; public class CustomUserValidator\u0026lt;TUser\u0026gt; : IIdentityValidator\u0026lt;TUser\u0026gt; where TUser : class, Microsoft.AspNet.Identity.IUser { private static readonly Regex EmailRegex = new Regex(@\u0026quot;^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}$\u0026quot;, RegexOptions.Compiled | RegexOptions.IgnoreCase); private readonly UserManager\u0026lt;TUser\u0026gt; _manager; public CustomUserValidator() { } public CustomUserValidator(UserManager\u0026lt;TUser\u0026gt; manager) { _manager = manager; } public async Task\u0026lt;IdentityResult\u0026gt; ValidateAsync(TUser item) { var errors = new List\u0026lt;string\u0026gt;(); if (!EmailRegex.IsMatch(item.UserName)) errors.Add(\u0026quot;Enter a valid email address.\u0026quot;); if (_manager != null) { var otherAccount = await _manager.FindByNameAsync(item.UserName); if (otherAccount != null \u0026amp;\u0026amp; otherAccount.Id != item.Id) errors.Add(\u0026quot;Select a different email address. An account has already been created with this email address.\u0026quot;); } return errors.Any() ? IdentityResult.Failed(errors.ToArray()) : IdentityResult.Success; } } UPDATE 03/28/2014: As John Holliday points out, Identity 2.0 requires that the TUser be a reference type, so where TUser : Microsoft.AspNet.Identity.IUser was updated to where TUser : class, Microsoft.AspNet.Identity.IUser. Thanks John!\nThis validator ensures that the UserName field is set to an email address. It also optionally ensures that the email address is not being used by another account.\nUPDATE 11/17/2013: I just found out that ValidateAsync is called at times other than when creating a new user, such as when adding external logins with UserManager.AddLoginAsync. This caused errors to occur with the original code because it thought it found duplicates. The fix this, \u0026amp;\u0026amp; otherAccount.Id != item.Id has been added when checking for duplicates.\nIn order to use this validator, just set your UserManager.UserValidator to a new instance of it.\n[Authorize] public class AccountController : Controller { public AccountController() : this(new UserManager\u0026lt;ApplicationUser\u0026gt;(new UserStore\u0026lt;ApplicationUser\u0026gt;(new ApplicationDbContext()))) { UserManager.UserValidator = new CustomUserValidator\u0026lt;ApplicationUser\u0026gt;(UserManager); } ","permalink":"https://blachniet.github.io/posts/email-addresses-as-user-names-in-asp-net-identity/","summary":"It\u0026rsquo;s common for web applications to use email addresses instead of user names to distinguish users. However, if you are using ASP.NET Identity, you have probably noticed that it has UserName built into the IUser interface. Since Identity assumes that this is the distinguishing field for the user, it\u0026rsquo;s not crazy to think that it might be a good place to drop the email address. In order to have Identity allow an email address in this field, you will need to write a custom IIdentityValidator.","title":"Email Addresses as User Names in ASP.NET Identity"},{"content":"I\u0026rsquo;m not proud to admit that I spent hours trying to figure out this very simple problem. The goal was simple: when a user logs into my web application using Google authentication, I want to be able to grab their email address so I can store it as part of their user profile. As expected, this is very simple.\nI\u0026rsquo;m assuming you\u0026rsquo;ve already enabled Google authentication by uncommenting app.UseGoogleAuthentication() in your Startup.Auth.cs.\nTo get the email address, just hop on over to AccountController.ExternalLoginCallback. Here, after the user has successfully authenticated with Google, you can grab the external identity. That external identity has some default claims in it, one of which (for Google Authentication at least) is the user\u0026rsquo;s email address.\n // // GET: /Account/ExternalLoginCallback [AllowAnonymous] public async Task\u0026lt;ActionResult\u0026gt; ExternalLoginCallback(string returnUrl) { var loginInfo = await AuthenticationManager.GetExternalLoginInfoAsync(); if (loginInfo == null) { return RedirectToAction(\u0026quot;Login\u0026quot;); } var externalIdentity = await AuthenticationManager .GetExternalIdentityAsync(DefaultAuthenticationTypes.ExternalCookie); var emailClaim = externalIdentity.Claims.FirstOrDefault(x =\u0026gt; x.Type.Equals( \u0026quot;http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\u0026quot;, StringComparison.OrdinalIgnoreCase)); var emailAddress = emailClaim != null ? emailClaim.Value : null; // Remainder of method excluded for brevity... The email address claim is identified by the http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress type. If you find that claim, it\u0026rsquo;s Value is the user\u0026rsquo;s email address.\nIf you find that you need other information that is not provided in the external identity by default, John Palmer has some great examples for adding claims on the IdentityUserPropertiesSample GitHub project.\n","permalink":"https://blachniet.github.io/posts/email-address-in-mvc-5-google-authentication/","summary":"I\u0026rsquo;m not proud to admit that I spent hours trying to figure out this very simple problem. The goal was simple: when a user logs into my web application using Google authentication, I want to be able to grab their email address so I can store it as part of their user profile. As expected, this is very simple.\nI\u0026rsquo;m assuming you\u0026rsquo;ve already enabled Google authentication by uncommenting app.UseGoogleAuthentication() in your Startup.","title":"Email Address in MVC 5 Google Authentication"},{"content":"If this is happening to you, it could be because you are using the Intel HD Graphics drivers. You\u0026rsquo;ll certainly notice this issue if you use Sublime Text since it uses the same key combination to expand the multiline editing.\nTo resolve the issue you can either change Intel\u0026rsquo;s keys or turn them off entirely.\n  Right click on you desktop and select Graphics Properties in the context menu.\n\n  Select Options\n\n  Either turn the hot keys off, or change their bindings.\n\n  ","permalink":"https://blachniet.github.io/posts/ctrlaltarrow-flips-my-screen/","summary":"If this is happening to you, it could be because you are using the Intel HD Graphics drivers. You\u0026rsquo;ll certainly notice this issue if you use Sublime Text since it uses the same key combination to expand the multiline editing.\nTo resolve the issue you can either change Intel\u0026rsquo;s keys or turn them off entirely.\n  Right click on you desktop and select Graphics Properties in the context menu.","title":"Ctrl+Alt+Arrow Flips My Screen"},{"content":"I have a few applications installed on my home server that are user-level applications, meaning that a user must be logged on in order for the application to start. I don\u0026rsquo;t want to have to manually log on ever time my server restarts, so I set up automatic log on for the server. Here\u0026rsquo;s how to do it.\n Press Windows Key + R to launch the run dialog. Type in netplwiz and click OK.  In the dialog disable Users must enter a username and password to use this computer.\n Click Apply and a dialog will appear. Enter the user name and password of the account that you would like to be automatically logged in.\n  That\u0026rsquo;s all there is to it. Having an automatic log on does not change the need to log on when using remote desktop to log on. I would suggest that you automatically log on to a standard account, not an admin account. That way if someone does sit down in front of the computer and start messing with things they shouldn\u0026rsquo;t, they can only do so much damage.\n","permalink":"https://blachniet.github.io/posts/windows-automatic-log-on/","summary":"I have a few applications installed on my home server that are user-level applications, meaning that a user must be logged on in order for the application to start. I don\u0026rsquo;t want to have to manually log on ever time my server restarts, so I set up automatic log on for the server. Here\u0026rsquo;s how to do it.\n Press Windows Key + R to launch the run dialog. Type in netplwiz and click OK.","title":"Windows Automatic Log On"},{"content":"You may have noticed that before you log on to you office PC that you are required to press CTRL+ALT+DELETE before entering you user name and password. Apparently, this is actually a security feature. According to this Windows support article (which this post is based on), [w]hen secure logon is enabled, no other program (such as a virus or spyware) can intercept your user name and password as you enter it.. To enable secure log on, follow the steps below.\n Hit Windows Key + R to launch the run dialog, and enter netplwiz and click OK Click the Advanced tab and enable Require users to press Ctrl+Alt+Delete and then click OK]  \n","permalink":"https://blachniet.github.io/posts/windows-secure-logon/","summary":"You may have noticed that before you log on to you office PC that you are required to press CTRL+ALT+DELETE before entering you user name and password. Apparently, this is actually a security feature. According to this Windows support article (which this post is based on), [w]hen secure logon is enabled, no other program (such as a virus or spyware) can intercept your user name and password as you enter it.","title":"Windows Secure Logon"},{"content":"Source on GitHub: https://github.com/blachniet/blachniet-psutils\nHave you ever wanted to send out email alerts when an particular event appears in the Windows Event Log? The Task Scheduler provides the ability to send out emails when an event is logged, but it doesn\u0026rsquo;t allow for SMTP servers that require any sort of authentication. I wanted to send the emails from a Gmail , so created a PowerShell script to handle this.\nThe script takes in quite a few required parameters in order to make it more flexible and reusable, so it may seem a little complicated at first. The mandatory parameters are Source, SmtpUser, SmtpPassword, MailFrom, and MailTo. All the other parameters have sensible defaults for sending emails through Gmail. You could customize these to be able to send email through some other provider if you needed to. Use Get-Help to get more information about all the parameters as well as some example usages.\nGet-Help Send-EventEntryEmail --detailed A scheduled task is still used to fire off the script, but the script handles getting the event entry information and sending off the email. Just create a new scheduled task with a trigger set to On an event. You can configure the trigger to watch for particular content in an event, events from a particular source, etc.\nThe implementation is actually pretty simple. To start off, I pull the latest event entries with Get-EventLog, and filter the results based on the script parameters (Source, Newest, LogName).\n# Get the event entries. $eventEntries = Get-EventLog -LogName $LogName -Source $Source -Newest $Newest -EntryType $EntryType $eventEntries is an array of System.Diagnostics.EventLogEntry. I then build a html table row for each of these entries and include the time the event was generated, the type of event, and the message associated with the event.\n# Create a table row for each entry. $rows = \u0026quot;\u0026quot; foreach ($eventEntry in $eventEntries){ $rows += @\u0026quot; \u0026lt;tr\u0026gt; \u0026lt;td style=\u0026quot;text-align: center; padding: 5px;\u0026quot;\u0026gt;$($eventEntry.TimeGenerated)\u0026lt;/td\u0026gt; \u0026lt;td style=\u0026quot;text-align: center; padding: 5px;\u0026quot;\u0026gt;$($eventEntry.EntryType)\u0026lt;/td\u0026gt; \u0026lt;td style=\u0026quot;padding: 5px;\u0026quot;\u0026gt;$($eventEntry.Message)\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026quot;@ } To finish things up, just create a new MailMessage and SmtpClient and send of the email.\n# Create the email. $email = New-Object System.Net.Mail.MailMessage( $MailFrom , $MailTo ) $email.Subject = $Subject $email.IsBodyHtml = $true $email.Body = @\u0026quot; \u0026lt;table style=\u0026quot;width:100%;border\u0026quot;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th style=\u0026quot;text-align: center; padding: 5px;\u0026quot;\u0026gt;Time\u0026lt;/th\u0026gt; \u0026lt;th style=\u0026quot;text-align: center; padding: 5px;\u0026quot;\u0026gt;Type\u0026lt;/th\u0026gt; \u0026lt;th style=\u0026quot;text-align: center; padding: 5px;\u0026quot;\u0026gt;Message\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; $rows \u0026lt;/table\u0026gt; \u0026quot;@ # Send the email. $SMTPClient=New-Object System.Net.Mail.SmtpClient( $SmtpServer , $SmtpPort ) $SMTPClient.EnableSsl=$true $SMTPClient.Credentials=New-Object System.Net.NetworkCredential( $SmtpUser , $SmtpPassword ); $SMTPClient.Send( $email ) Hopefully others will find this script useful. Feel free to customize the script if you want. You can find it in my GitHub project https://github.com/blachniet/blachniet-psutils.\n","permalink":"https://blachniet.github.io/posts/event-log-email-alerts-with-powershell/","summary":"Source on GitHub: https://github.com/blachniet/blachniet-psutils\nHave you ever wanted to send out email alerts when an particular event appears in the Windows Event Log? The Task Scheduler provides the ability to send out emails when an event is logged, but it doesn\u0026rsquo;t allow for SMTP servers that require any sort of authentication. I wanted to send the emails from a Gmail , so created a PowerShell script to handle this.\nThe script takes in quite a few required parameters in order to make it more flexible and reusable, so it may seem a little complicated at first.","title":"Event Log Email Alerts with PowerShell"},{"content":"Apparently Google has started using WebP image format with the redesign of the Google Play store. Only Chrome supports the image format at the moment, so if you are using another browser you will get the standard PNG.\nI did a quick test to see how big of a difference it actually makes. I grabbed the icon from the Chrome Browser page on Chrome and then on IE. When downloaded, Chrome\u0026rsquo;s WebP file was 11.5KB vs 93.8KB for IE\u0026rsquo;s PNG!\nAfter seeing that, I decided to give it a shot with my icon. You can grab the tools to convert PNGs to WebP files from the WebP download page. The cwebp utility does this particular conversion. It takes an input, output and quality (0-100).\ncwebp [options] -q quality input.png -o output.webp I tried the logo at some of the different quality levels to see how much of a difference they make in size. The original size of my PNG was 3625 bytes. As you can see in the chart below, there\u0026rsquo;s a huge difference when you drop the quality to 90, and then there is a more gradual drop as you go any lower. For my logo, there was noticeable changes in quality at the 80 mark, but that will vary depending on your image.\nSince WebP is not yet natively supported by browsers other than Chrome, you probably don\u0026rsquo;t want to jump to it quite yet. You can still reduce the size of you PNGs by using optimizers like TinyPNG. As an example, I used TinyPNG to optimize the same logo and it reduced the size from 3625 bytes to 1991 bytes, which would put you just a little above the 90 WebP with no noticeable difference in quality.\nIf you just can\u0026rsquo;t wait for other browsers to support the format (assuming they ever do), check out the WebPJS library, which adds support for the new format to some browsers via a JavaScript library.\n","permalink":"https://blachniet.github.io/posts/welcoming-webp/","summary":"Apparently Google has started using WebP image format with the redesign of the Google Play store. Only Chrome supports the image format at the moment, so if you are using another browser you will get the standard PNG.\nI did a quick test to see how big of a difference it actually makes. I grabbed the icon from the Chrome Browser page on Chrome and then on IE. When downloaded, Chrome\u0026rsquo;s WebP file was 11.","title":"Welcoming WebP"},{"content":"This post and sample application demonstrate a possible issue with the way that Entity Framework loads the embedded metadata resources (csdl, msl, ssdl) for the entity model.\nIn my application, all assemblies in some specific directory are discovered and loaded at runtime. Think of it as a sort of plugin model. The application allows for multiple versions of the same assembly to be loaded, and each dll follows the [AssemblyName].[Version].dll naming scheme. One of my assemblies that is discovered and loaded at runtime contains an EF entity model and uses it to perform some queries against the database. I will refer to this assembly as EFContainer. Whenever I make changes to the database schema, I update the model, and create a new version of EFContainer. In order to make sure I load the correct embedded metadata resources, my connection string uses the full assembly name to specify the location of the resoruces as opposed to the default wildcard (res://*/FooModel.csdl|...):\n\u0026lt;add name=\u0026quot;FooContext\u0026quot; connectionString=\u0026quot;metadata= res://EFContainer, Version=2.0.0.0, Culture=neutral, PublicKeyToken=null/FooModel.csdl| res://EFContainer, Version=2.0.0.0, Culture=neutral, PublicKeyToken=null/FooModel.ssdl| res://EFContainer, Version=2.0.0.0, Culture=neutral, PublicKeyToken=null/FooModel.msl; provider=.../\u0026gt; Herein lies the problem. The full assembly name in that conenction string does not seem to be honored. Instead it seems that the embedded metadata of the assembly loaded first is used.\nTo demonstrate this issue, I\u0026rsquo;ve created this sample application which uses EFContainer.1.0.0.0.dll and EFContainer.2.0.0.0.dll. Version 1.0 contains outdated metadata information and is incompatible with the current database schema. Version 2.0 contains updated metadata that is compatible with the current database schema. The updated schema in 2.0 consists of a new column in the Bars table, Rating, which is an INT with a NOT NULL constraint. As we would expect, using the metadata in version 1.0 throws an exception because the Rating column does not exist in the metadata and it does not allow for null values.\nSince I am including the full assembly name of version 2.0 in my connection string, I would expect that the resources from the version 2.0 assembly would be used. However, it seems like it actually just uses the resources from whichever assembly was loaded first. You can test this out for yourslf in the EFMultiSchemaVersionTest\u0026rsquo;s Program.cs. If you run it as-is, it will throw an exception indicating that Rating does not allow for null values. If you uncomment the line containing filesToLoad = filesToLoad.Reverse().ToArray();, causing the v2.0 assembly to be loaded before the v1.0, the application will complete without error.\nIn the meantime I have resorted to setting the Metadata Artifact Processing to Copy to Output Directory, and putting those metadata files in a public location that the assemblies can access at runtime. If anyone notices something that I\u0026rsquo;m missing, please let me know. I would prefer to embed those resources in the assembly.\n CodePlex Issue Example Application on GitHub  ","permalink":"https://blachniet.github.io/posts/referencing-ef-resources-in-a-specific-assembly/","summary":"This post and sample application demonstrate a possible issue with the way that Entity Framework loads the embedded metadata resources (csdl, msl, ssdl) for the entity model.\nIn my application, all assemblies in some specific directory are discovered and loaded at runtime. Think of it as a sort of plugin model. The application allows for multiple versions of the same assembly to be loaded, and each dll follows the [AssemblyName].[Version].dll naming scheme.","title":"Referencing EF Resources in a Specific Assembly"},{"content":"I usually use BitBucket or GitHub to host my git repositories, but sometimes I need to host them on some internal network. In this post I will show you how to host those repositories on a Windows share.\nI\u0026rsquo;m going to cover two scenarios: 1) You already have a git repository 2) You are starting fresh with a brand new repository. For both of these tracks I\u0026rsquo;m going to assume that you already have a Windows share set up on some remote computer, and that you have the read and write permissions on that share. To see more detailed information about hosting git on a server, check out section 4.2 Git on the Server - Getting Git on a Server of the Pro Git book.\nI Already Have a Repository Clone your repo (my_project) into a new bare repository (my_project.git):\n\u0026gt; git clone --bare my_project my_project.git Move the bare repository to the Windows share.\nAdd a remote to my_project that points to my_project.git. You can name your remote something other than origin if you would like. Note that forward slashes are used, not backslashes.\n\u0026gt; git remote add origin //{ServerNameOrIp}/{ShareName}/my_project.git You\u0026rsquo;re all set up. You can now push and pull from that remote repository. If you need to clone that repo later, just use\n\u0026gt; git clone //{ServerNameOrIp}/{ShareName}/my_project.git` I Don\u0026rsquo;t Have a Repository Yet, I\u0026rsquo;m Starting Fresh Create a new bare repository.\n\u0026gt; mkdir my_project.git \u0026gt; cd my_project.git \u0026gt; git init --bare Move the bare repository to the Windows share.\nClone the repository. Note that forward slashes are used, not backslashes.\n\u0026gt; git clone //{ServerNameOrIp}/{ShareName}/my_project.git` You\u0026rsquo;re all set up. You can now push and pull from that remote repository.\n","permalink":"https://blachniet.github.io/posts/host-git-repositories-on-a-windows-share/","summary":"I usually use BitBucket or GitHub to host my git repositories, but sometimes I need to host them on some internal network. In this post I will show you how to host those repositories on a Windows share.\nI\u0026rsquo;m going to cover two scenarios: 1) You already have a git repository 2) You are starting fresh with a brand new repository. For both of these tracks I\u0026rsquo;m going to assume that you already have a Windows share set up on some remote computer, and that you have the read and write permissions on that share.","title":"Host Git Repositories on a Windows Share"},{"content":"So I finally got the time to install my 3TB hard drive in my home server, but ran into some problems. After installing the drive, it showed only 746GB available on the drive. I know I\u0026rsquo;m never going to see 3000GB (because hard drive manufacturers are liars) but this was way below the expected size.\nLuckily, I\u0026rsquo;m not the only one that\u0026rsquo;s run into this issue. My first instict was to go to Disk Management to try to extend the volume, but when I got in there, it only showed the single volume with 746GB, and no unallocated space. To resolve this, I did the following:\n Downloaded and installed Segate\u0026rsquo;s DiscWizard Opened the DiscWizard - Just ran it, didn\u0026rsquo;t do anything in it. Closed and reopened Disk Management My drive now showed unallocated space, so I extended the existing volume to include the rest of the space.  I\u0026rsquo;m not really sure why this works, but it did. If you run into issues, try using the DiscWizard to try to extend the volume.\n","permalink":"https://blachniet.github.io/posts/746gb-in-a-3tb-seagate-drive/","summary":"So I finally got the time to install my 3TB hard drive in my home server, but ran into some problems. After installing the drive, it showed only 746GB available on the drive. I know I\u0026rsquo;m never going to see 3000GB (because hard drive manufacturers are liars) but this was way below the expected size.\nLuckily, I\u0026rsquo;m not the only one that\u0026rsquo;s run into this issue. My first instict was to go to Disk Management to try to extend the volume, but when I got in there, it only showed the single volume with 746GB, and no unallocated space.","title":"746GB in a 3TB Seagate Drive"},{"content":"It doesn\u0026rsquo;t look like AppHarbor is supporting TypeScript yet, which makes sense considering that TypeScript is in preview. So if you want to use AppHarbor to build your TypeScript files as part of its continuous integration/deployment, you\u0026rsquo;ll have to do a little bit of manual work. Before going through the steps below, make sure you have followed the steps in the Compile-on-Save TypeScript post.\nCopy C:\\Program Files\\MSBuild\\Microsoft\\Visual Studio\\v11.0\\TypeScript and C:\\Program Files\\Microsoft SDKs\\TypeScript into some place in your repository. I copied the contents of both directories into a directory at the top of my repository, Tools\\Typescript.\nIn your copied Microsoft.TypeScript.targets, replace\n\u0026lt;Exec Command=\u0026quot;tsc $(TypeScriptBuildConfigurations) @(TypeScriptCompile -\u0026gt;'\u0026quot;%(fullpath)\u0026quot;', ' ')\u0026quot; /\u0026gt; with\n\u0026lt;Exec Command=\u0026quot;..\\Tools\\TypeScript\\tsc.exe $(TypeScriptBuildConfigurations) @(TypeScriptCompile -\u0026gt;'\u0026amp;quot;%(fullpath)\u0026amp;quot;', ' ')\u0026quot; /\u0026gt; In your .csproj file, replace\n\u0026lt;Import Project=\u0026quot;$(MSBuildExtensionsPath32)\\Microsoft\\VisualStudio\\v$(VisualStudioVersion)\\TypeScript\\Microsoft.TypeScript.targets\u0026quot; /\u0026gt; with\n\u0026lt;Import Project=\u0026quot;..\\Tools\\TypeScript\\Microsoft.TypeScript.targets\u0026quot; /\u0026gt; If you put the copied files somewhere else in your repository, make sure you modify the paths in the replacement code appropriately. Commit/push your changes and AppHarbor should successfully build your project.\n","permalink":"https://blachniet.github.io/posts/typescript-0-8-2-on-appharbor/","summary":"It doesn\u0026rsquo;t look like AppHarbor is supporting TypeScript yet, which makes sense considering that TypeScript is in preview. So if you want to use AppHarbor to build your TypeScript files as part of its continuous integration/deployment, you\u0026rsquo;ll have to do a little bit of manual work. Before going through the steps below, make sure you have followed the steps in the Compile-on-Save TypeScript post.\nCopy C:\\Program Files\\MSBuild\\Microsoft\\Visual Studio\\v11.0\\TypeScript and C:\\Program Files\\Microsoft SDKs\\TypeScript into some place in your repository.","title":"TypeScript 0.8.2 on AppHarbor"},{"content":"There are 2 particularly good stored procedures in SQL Server for getting information about a particular table.\nsp_columns returns detailed information about each of the columns in the table. Thanks to Vincent Ramdhanie StackOverflow answer\nsp_columns @tablename sp_help returns detailed information about the entire table including the columns and constraints. Thanks to Brannon for his StackOverflow answer\nsp_help @tablename ","permalink":"https://blachniet.github.io/posts/describing-tables-in-mssql/","summary":"There are 2 particularly good stored procedures in SQL Server for getting information about a particular table.\nsp_columns returns detailed information about each of the columns in the table. Thanks to Vincent Ramdhanie StackOverflow answer\nsp_columns @tablename sp_help returns detailed information about the entire table including the columns and constraints. Thanks to Brannon for his StackOverflow answer\nsp_help @tablename ","title":"Describing Tables in MSSQL"},{"content":"If you are using GitHub for Windows with non-GitHub repositories, you may have run into the limitation that you cannot sync using an SSH key with a passphrase. One workaround is to push/pull from the shell, but if don\u0026rsquo;t mind removing the passphrase from the key, use the following command.\nssh-keygen -p -P \u0026quot;my_old_password\u0026quot; -N “” -f my_key_file_name After you remove the passphrase you should be able to sync from within the GitHub for Windows client.\n","permalink":"https://blachniet.github.io/posts/remove-a-passphrase-from-an-ssh-key/","summary":"If you are using GitHub for Windows with non-GitHub repositories, you may have run into the limitation that you cannot sync using an SSH key with a passphrase. One workaround is to push/pull from the shell, but if don\u0026rsquo;t mind removing the passphrase from the key, use the following command.\nssh-keygen -p -P \u0026quot;my_old_password\u0026quot; -N “” -f my_key_file_name After you remove the passphrase you should be able to sync from within the GitHub for Windows client.","title":"Remove a Passphrase from a SSH Key"},{"content":"The default DbContext generated with your Entity Data Model has one constructor that takes no arguments. If you need to build the connection string programatically and pass it to the DbContext, you will have to add your own constructor. However, if you do this in the generated file, {ModelName}.Context.cs, you will lose your changes the next time the {ModelName}.Context.tt script is run.\nThe solution is to edit the script, {ModelName}.Context.tt. Just add the following code after the code to generate the default constructor (around line 71). Run the script (Ctrl+S), and verify that the new constructor exists in {ModelName}.Context.cs.\n public \u0026lt;#=code.Escape(container)#\u0026gt;(string nameOrConnectionString) : base(nameOrConnectionString) { \u0026lt;# if (!loader.IsLazyLoadingEnabled(container)) { #\u0026gt; this.Configuration.LazyLoadingEnabled = false; \u0026lt;# } #\u0026gt; } ","permalink":"https://blachniet.github.io/posts/pass-a-connection-string-to-a-generated-dbcontext/","summary":"The default DbContext generated with your Entity Data Model has one constructor that takes no arguments. If you need to build the connection string programatically and pass it to the DbContext, you will have to add your own constructor. However, if you do this in the generated file, {ModelName}.Context.cs, you will lose your changes the next time the {ModelName}.Context.tt script is run.\nThe solution is to edit the script, {ModelName}.Context.tt. Just add the following code after the code to generate the default constructor (around line 71).","title":"Pass a Connection String to a Generated DbContext"},{"content":"Lately I\u0026rsquo;ve been working on improving my home network, so I\u0026rsquo;ve decided to write up some posts on my experience. Most of them will probably be pretty short and reference other sites and tutorials, but I plan on pointing out some of the issues, tips and tricks I\u0026rsquo;ve discovered along the way.\nHere\u0026rsquo;s some of the things that I\u0026rsquo;ll be going over. As I make posts about each of these, I\u0026rsquo;ll place links to them here.\n  Planning\n Network Naming Schemes Diagram Your Network Establish Logins    Features\n File Share Server - Store your pictures, music, and other files in one place and share them with all your devices. File Backup Server - Periodically back up your computers to a server. Media Server - Stream movies, music and tv shows from a server to all your devices. VPN - Access you network resources from anywhere Monitoring Server - Monitor your IP cameras and start recording when motion is detected. DVD Ripping Server - Use a server to run CPU intensive tasks such as ripping DVDs.    ","permalink":"https://blachniet.github.io/posts/home-network-guide/","summary":"Lately I\u0026rsquo;ve been working on improving my home network, so I\u0026rsquo;ve decided to write up some posts on my experience. Most of them will probably be pretty short and reference other sites and tutorials, but I plan on pointing out some of the issues, tips and tricks I\u0026rsquo;ve discovered along the way.\nHere\u0026rsquo;s some of the things that I\u0026rsquo;ll be going over. As I make posts about each of these, I\u0026rsquo;ll place links to them here.","title":"Home Network Guide"},{"content":"One of the first things you should do when setting up your home network is decide on a naming scheme. Have fun with this part. You will need to pick a workgroup name as well as names for each of your computers/devices.\nI like the TV show South Park, so I decided to base my network device names on South Park characters. My workgroup name is SOUTHPARK, my main desktop is named STAN, my wife\u0026rsquo;s computer is named WENDY, and my server is named BUTTERS. You might want to base your naming scheme on Mario Kart characters, Transformers, Zelda Characters, Chevy cars, horse breeds, Game of Thrones houses, or something else you are interested in.\nIt\u0026rsquo;s important that you be consistent with the capitalization of your device names. I generally stick with all-caps names. You\u0026rsquo;ll also want to name your WiFi network. I named mine the same as my workgroup.\nWindows 7 - Set Computer Name/Workgroup   Open the Start Menu, right click Computer, and click Properties\n  In the left column, select Advanced system settings   Select the Computer Name tab and click the Change\u0026hellip; button.   Insert your desired computer name and workgroup name   Mac - Set Computer Name/Workgroup   Open System Preferences | Network   Select the Advanced option   Select the WINS tab and enter your desired computer name and workgroup   Ubuntu - Set Computer Name/Workgroup I don\u0026rsquo;t have a Ubuntu system up and running at the moment, so I\u0026rsquo;ll point you to an article over on Liberian Geek for instructions on this one: Windows 7 vs. Ubuntu 12.04: How to Change System Workgroup. The Ubuntu portion is about half-way down the page.\nAndroid/iOS - Set Computer Name/Workgroup This doesn\u0026rsquo;t seem to be possible with most Android/iOS devices without rooting or jail braking them. Sorry if I got your hopes up.\n","permalink":"https://blachniet.github.io/posts/network-naming-schemes/","summary":"One of the first things you should do when setting up your home network is decide on a naming scheme. Have fun with this part. You will need to pick a workgroup name as well as names for each of your computers/devices.\nI like the TV show South Park, so I decided to base my network device names on South Park characters. My workgroup name is SOUTHPARK, my main desktop is named STAN, my wife\u0026rsquo;s computer is named WENDY, and my server is named BUTTERS.","title":"Network Naming Schemes"},{"content":"Normally if you need to restart a WPF application programatically, you would use the following code:\nprivate void Restart() { System.Diagnostics.Process.Start( Application.ResourceAssembly.Location); Application.Current.Shutdown(); } If you have a ClickOnce WPF application, you do not want to do this. In order to understand why, you need to understand how a ClickOnce application is normally launched.\nThe shortcut you click on in the Start menu is not a normal shortcut to an executable. It is actually an appref-ms file, which defines the entry point and location of the application. When you launch using this file, all the parameters in ApplicationDeployment are correctly initialized, and ApplicationDeployment.IsNetworkDeployed is set to true.\nHowever, if you use the code above, you launch using the application\u0026rsquo;s entry executable instead of the appref-ms. This will cause ApplicationDeployment.IsNetworkDeployed to be false, and the properties will not be initialized as if they were a ClickOnce deployment.\nAccording to Rob Relyea\u0026rsquo;s post, Application.Restart() for WPF, you can get the desired functionality with the following code. I have not had a chance to verify that this works as expected, but I will update this post as soon as I do.\nUPDATE: I\u0026rsquo;ve verified that the following code does work properly. Sadly, you will need a reference to the System.Windows.Forms.dll.\nprivate void Restart() { // from System.Windows.Forms.dll System.Windows.Forms.Application.Restart(); Application.Current.Shutdown(); } ","permalink":"https://blachniet.github.io/posts/how-not-to-restart-a-clickonce-application/","summary":"Normally if you need to restart a WPF application programatically, you would use the following code:\nprivate void Restart() { System.Diagnostics.Process.Start( Application.ResourceAssembly.Location); Application.Current.Shutdown(); } If you have a ClickOnce WPF application, you do not want to do this. In order to understand why, you need to understand how a ClickOnce application is normally launched.\nThe shortcut you click on in the Start menu is not a normal shortcut to an executable.","title":"How Not To Restart A ClickOnce Application"},{"content":" I\u0026rsquo;ve put together a very simple borderless WPF window that complements Microsoft\u0026rsquo;s Metro style quite well. I won\u0026rsquo;t go into a lot of detail here, since the code does most of the talking, but I will go over the important parts here. If you want to skip right to the source code, you can find it over at GitHub: http://bit.ly/N7afKQ\nStarting with a fresh window set and the window properties WindowStyle=\u0026quot;None\u0026quot;, AllowsTransparency=\u0026quot;True\u0026quot;, and ResizeMode=\u0026quot;CanResizeWithGrip\u0026quot;. This will give you your starting borderless window that is resizable from the bottom-right corner.\n\u0026lt;Window x:Class=\u0026quot;SimpleBorderlessWindow.MainWindow\u0026quot; xmlns=\u0026quot;http://schemas.microsoft.com/winfx/2006/xaml/presentation\u0026quot; xmlns:x=\u0026quot;http://schemas.microsoft.com/winfx/2006/xaml\u0026quot; Title=\u0026quot;MainWindow\u0026quot; Height=\u0026quot;562\u0026quot; Width=\u0026quot;1000\u0026quot; WindowStyle=\u0026quot;None\u0026quot; AllowsTransparency=\u0026quot;True\u0026quot; ResizeMode=\u0026quot;CanResizeWithGrip\u0026quot;\u0026gt; A DockPanel is used to group together all the top elements (window title, draggable area, close/minimize buttons). Two TextBlocks with Hyperlinks docked to the right make up the minimize and close buttons. A StatusBar with the window title makes up the rest of the area, and defines the draggable area.\n\u0026lt;DockPanel\u0026gt; \u0026lt;DockPanel DockPanel.Dock=\u0026quot;Top\u0026quot;\u0026gt; \u0026lt;TextBlock DockPanel.Dock=\u0026quot;Right\u0026quot; VerticalAlignment=\u0026quot;Center\u0026quot;\u0026gt; \u0026lt;Hyperlink Click=\u0026quot;TriggerClose\u0026quot; Style=\u0026quot;{StaticResource WindowIconStyle}\u0026quot;\u0026gt;r\u0026lt;/Hyperlink\u0026gt; \u0026lt;/TextBlock\u0026gt; \u0026lt;TextBlock DockPanel.Dock=\u0026quot;Right\u0026quot; VerticalAlignment=\u0026quot;Center\u0026quot;\u0026gt; \u0026lt;Hyperlink Click=\u0026quot;TriggerMinimize\u0026quot; Style=\u0026quot;{StaticResource WindowIconStyle}\u0026quot;\u0026gt;0\u0026lt;/Hyperlink\u0026gt; \u0026lt;/TextBlock\u0026gt; \u0026lt;StatusBar Background=\u0026quot;Transparent\u0026quot; MouseDoubleClick=\u0026quot;TriggerMaximize\u0026quot; MouseMove=\u0026quot;TriggerMoveWindow\u0026quot;\u0026gt; \u0026lt;TextBlock DockPanel.Dock=\u0026quot;Left\u0026quot; Text=\u0026quot;SimpleBorderlessWindow\u0026quot; FontSize=\u0026quot;16\u0026quot;/\u0026gt; \u0026lt;/StatusBar\u0026gt; \u0026lt;/DockPanel\u0026gt; \u0026lt;Grid\u0026gt; \u0026lt;TextBlock Text=\u0026quot;Window Content Here\u0026quot; HorizontalAlignment=\u0026quot;Center\u0026quot; VerticalAlignment=\u0026quot;Center\u0026quot; /\u0026gt; \u0026lt;/Grid\u0026gt; \u0026lt;/DockPanel\u0026gt; A style is used to make the icons change color when the mouse is over them. This also removes the underlining, the hand cursor, and sets the font to Webdings which gives us some nice icons.\n\u0026lt;Window.Resources\u0026gt; \u0026lt;Style x:Key=\u0026quot;WindowIconStyle\u0026quot; TargetType=\u0026quot;{x:Type Hyperlink}\u0026quot;\u0026gt; \u0026lt;Setter Property=\u0026quot;FontFamily\u0026quot; Value=\u0026quot;Webdings\u0026quot; /\u0026gt; \u0026lt;Setter Property=\u0026quot;FontSize\u0026quot; Value=\u0026quot;16\u0026quot; /\u0026gt; \u0026lt;Setter Property=\u0026quot;Foreground\u0026quot; Value=\u0026quot;White\u0026quot; /\u0026gt; \u0026lt;Setter Property=\u0026quot;TextBlock.TextDecorations\u0026quot; Value=\u0026quot;{x:Null}\u0026quot; /\u0026gt; \u0026lt;Setter Property=\u0026quot;Cursor\u0026quot; Value=\u0026quot;{x:Null}\u0026quot; /\u0026gt; \u0026lt;Style.Triggers\u0026gt; \u0026lt;Trigger Property=\u0026quot;IsMouseOver\u0026quot; Value=\u0026quot;True\u0026quot;\u0026gt; \u0026lt;Setter Property=\u0026quot;Foreground\u0026quot; Value=\u0026quot;#ED5326\u0026quot; /\u0026gt; \u0026lt;/Trigger\u0026gt; \u0026lt;/Style.Triggers\u0026gt; \u0026lt;/Style\u0026gt; \u0026lt;/Window.Resources\u0026gt; Finally you just have to hook up the events in your code behind. OMG, Did he say code-behind!!! - yup. I love MVVM, and try to avoid code behind in general, but this functionality seems like it belongs in the view layer (which the code-behind falls into the view layer here). You can still easily hook this view up to a view model, and use everything else in the beautiful MVVM way.\npublic partial class MainWindow : Window { public MainWindow() { InitializeComponent(); } private void TriggerMoveWindow(object sender, MouseEventArgs e) { if (e.LeftButton == MouseButtonState.Pressed) { if (WindowState == System.Windows.WindowState.Maximized) { WindowState = System.Windows.WindowState.Normal; double pct = PointToScreen(e.GetPosition(this)).X / System.Windows.SystemParameters.PrimaryScreenWidth; Top = 0; Left = e.GetPosition(this).X - (pct * Width); } DragMove(); } } private void TriggerMaximize(object sender, MouseButtonEventArgs e) { if (WindowState == System.Windows.WindowState.Maximized) WindowState = System.Windows.WindowState.Normal; else if (WindowState == System.Windows.WindowState.Normal) WindowState = System.Windows.WindowState.Maximized; } private void TriggerClose(object sender, RoutedEventArgs e) { Close(); } private void TriggerMinimize(object sender, RoutedEventArgs e) { WindowState = System.Windows.WindowState.Minimized; } } The full source code can be found over at GitHub at http://bit.ly/N7afKQ. Please feel free to contribute or offer any suggestions.\n","permalink":"https://blachniet.github.io/posts/borderless-wpf-window/","summary":"I\u0026rsquo;ve put together a very simple borderless WPF window that complements Microsoft\u0026rsquo;s Metro style quite well. I won\u0026rsquo;t go into a lot of detail here, since the code does most of the talking, but I will go over the important parts here. If you want to skip right to the source code, you can find it over at GitHub: http://bit.ly/N7afKQ\nStarting with a fresh window set and the window properties WindowStyle=\u0026quot;None\u0026quot;, AllowsTransparency=\u0026quot;True\u0026quot;, and ResizeMode=\u0026quot;CanResizeWithGrip\u0026quot;.","title":"Borderless WPF Window"},{"content":"In the game that I’ve been working on recently I’ve been using this amazing little library called World Construction Kit (WCK). This framework has saved me tons of time and made setting up Box2D worlds and bodies really easy. Emanuele Feronato wrote up a nice getting started tutorial and there are 2 very useful pages on the GitHub Wiki:\n Box2d Flash Alchemy Port + World Construction Kit –Feronato World Construction Kit –github Wiki Box2D Flash Alchemy Port –github Wiki This is great material for getting started. This post is going to assume you have read these and have managed to get a sample up and running. Make sure you check out the demo folder in the source. This has all the source for the demo seen on the WCK homepage. Below are a few notes that would have made my life easier when working with WCK. Hopefully they will be helpful to you.  Initialization and Destruction The BodyShape class indirectly derives from Entity, which provides constructor and destructor functionality for stage instances based on when they are added or removed from the stage. When the instance is added to the stage, create()is called and when the instance is removed from the stage destroy() is called. This is important because BodyShape overrides create and calls createBody() as part of its creation process. When you extend the BodyShape class and need to modify the underlying b2Body object, you will need to make sure that you do not try to access the b2Body before it is created. You do this by overriding the createBody() method, and doing your object specific initialization after calling super.createBody();\noverride public function createBody():void { super.createBody(); b2body.SetUserData( new BodyData(BodyData.WALL, this)); } Initialization Order The order in which objects are initialized on stage is not guaranteed. The Entity class provides functions to allow you to make sure an object you are dependent on is properly created before you access it. Every Entity has an ensureCreate() and ensureDestroy(). These functions will call create() if it has not already been created, or destroy() if it has not already been destroyed.\nEvent Listeners Entity provides a function, listenWhileVisible(), which is an alternative way of adding event listeners. Any events added with this function will be automatically removed when the object\u0026rsquo;s destroy function is called. You can call stopListening() if you need to remove the listener before then. You don\u0026rsquo;t have to use this approach, but it does make life a little easier when you don\u0026rsquo;t have to worry about remembering to remove event listeners.\nFrame Code If you have already looked at the demo source in the WCK library, you may have noticed that there is frame code defining some of the bodies. If not, open up demo.fla and take a look at BodyShapeStuff\\PolyCustom. The body for this is being defined on the Layer 2 frame code. You are not required to define custom bodies on frame code, but it is an option you have. If the only reason you would need a class file is so that you could define the custom body, I would just put it on the frame code. If you are uncomfortable with this, don\u0026rsquo;t do it.\nScripts The script directory in the WCK source has some very useful JSFL scripts for use in Flash Pro. Open the directory and copy the files to your Flash pro user data directory (on Windows 7 it is C:\\Users[USERNAME]\\AppData\\Local\\Adobe[FLASH VERSION]\\en_US\\Configuration\\Commands\\WCK). Then when you open Flash Pro and click on the Commands menu item, you will be able to execute these scripts. Their function is pretty self explanatory, so try each of them out.\nUse V2 I know this was already mentioned in the github documentation, but it is very important. If you are used to the BorisTheBrave port or any other version of the Box2D library, then you are used to creating b2Vec2s. You need to break this habit when working with WCK and the Box2D Flash Alchemy Port. Only use V2. See the github wiki page here if you want a better explanation as to why you should do this.\n","permalink":"https://blachniet.github.io/posts/diving-into-world-construction-kit/","summary":"In the game that I’ve been working on recently I’ve been using this amazing little library called World Construction Kit (WCK). This framework has saved me tons of time and made setting up Box2D worlds and bodies really easy. Emanuele Feronato wrote up a nice getting started tutorial and there are 2 very useful pages on the GitHub Wiki:\n Box2d Flash Alchemy Port + World Construction Kit –Feronato World Construction Kit –github Wiki Box2D Flash Alchemy Port –github Wiki This is great material for getting started.","title":"Diving Into World Construction Kit"},{"content":" This post is a simple walkthrough for getting the latest Flare3D pre release (2.0.42) and the latest FlashDevelop (4.0.0 RC1) working together nicely. This post builds on an older how-to done by the Flare3D team which can be found here.\n  Get the latest FlashDevelop  Download FlashDevelop 4.0.0 RC1 here. Run the installer.  Make sure that you install Flex 4.5 SDK as part of the FlashDevelop installation. The installer will do this by default.      Get FlashPlayer 11  Download the latest FlashPlayer here. Note that Chrome updates its FlashPlayer automatically, and at the time of this posting, it was still at version 10.3. You may have to use a different browser, such as Firefox. See Nick H\u0026rsquo;s comment below on how to use FlashPlayer 11 in Chrome by disabling the default player plug-in.    Get Flare3D 2.0 Pre-Release  Sign up here to get a download link emailed to you. Unzip the downloaded files.    Configure the Project  Open flare3d_prerelease.as3proj in FlashDevelop Open the project properties (right click on the project and select \u0026ldquo;Properties\u0026quot;li\u0026gt; In the Output tab, Platform group box, select Flash Player version 11.0  \n  To automatically launch in Firefox:\n  In the Test Project group box (still Output tab) select \u0026lsquo;Run Custom Command\u0026hellip;\u0026rsquo;, click Edit, and enter \u0026lsquo;firefox.exe bin\\index.html\u0026rsquo;\n  Go to the SDK tab and make sure that Flex 4.5.1, AIR 2.7 is the selected SDK\n  \n Go to the Compiler Options and add \u0026lsquo;lib\\Flare3D_2042.swc\u0026rsquo; to the SWC Libraries array. Right click on \u0026lsquo;lib\\Flare3D_2042.swc\u0026rsquo; in the project tree and check \u0026lsquo;Add to Library\u0026rsquo;. Thanks to Philippe for this suggested change.  \n  Run an example  Open the \u0026lsquo;examples\u0026rsquo; folder in the project Right click on Test01_The_Basics1.as Click Document Class Build \u0026amp; Launch by pressing Ctrl+Enter  __\n  ","permalink":"https://blachniet.github.io/posts/flare3d-flashdevelop/","summary":"This post is a simple walkthrough for getting the latest Flare3D pre release (2.0.42) and the latest FlashDevelop (4.0.0 RC1) working together nicely. This post builds on an older how-to done by the Flare3D team which can be found here.\n  Get the latest FlashDevelop  Download FlashDevelop 4.0.0 RC1 here. Run the installer.  Make sure that you install Flex 4.5 SDK as part of the FlashDevelop installation.","title":"Flare3D \u0026 FlashDevelop Setup"},{"content":"In my last post we set up our EasyPHP server and created a MySQL database for our event manager. In this post we are going to create the Amfphp service to create, delete, and get events from the database. We are also going to create an ActionScript 3 class to talk to the PHP service.\nCreating the Service Your event service will be contained within a single PHP file in the Amfphp\\Services\\ directory. You can get to this by right clicking on the EasyPHP icon in your notification area and selecting Explore. Navigate to the Amfphp\\Services\\ directory and create your new PHP file, EventService.php. Instead of pasting all the code for this class in this post, I have uploaded the code here. The service is made up of a single PHP class, EventService. The first thing you will see in this class is helper functions to open and close the connection to the database.\nprivate function openDB(){ // Open the database connection. $db = mysql_connect(\u0026quot;localhost\u0026quot;, \u0026quot;root\u0026quot;, \u0026quot;yourPassword\u0026quot;); //Select the event database. mysql_select_db(\u0026quot;event_manager_db\u0026quot;, $db); return $db; } private function closeDB($db){ mysql_close($db); } You will then see the code for our three service functions, CreateEvent, DeleteEvent, and GetEvents. Notice that opeDB() is always called before any SQL commands are issued and that closeDB() is called after the SQL connection is no longer needed. Most everything in between is normal MySQL/PHP interaction. CreateEvent() takes in the name and location of the event. These are then used in the SQL INSERT statement. mysql_query returns true/false for INSERT statements, so we build our success/failure return based on this value. DeleteEvent() is fairly straightforward as well. If you remember from the last post, we made the name of events UNIQUE in the database, so the delete function uses this to delete the proper event. mysql_query returns true/false for SQL DELETE queries as well. GetEvents() is a little different in that the SQL SELECT query returns row data instead of a true/false value. We iterate through this row data and build an array of event data to return. It is important that you make sure to release the row data after copying it into your array by calling mysql_free_result($result);.\nwhile ($row = mysql_fetch_assoc($result)){ array_push($event_array, $row); } mysql_free_result($result); After your service is complete you should be able to go to http://127.0.0.1:8888/Amfphp/ and see your EventService. Here you can test the service by calling the 3 public functions implemented in the service class.\nActionScript Data Manager We can now create an ActionScript class that will handle communication between the PHP service and the rest of the ActionScript. I have posted the source for this project here, which includes the FlashDevelop project file, an EventDataManager class, and a test document class (Main.as). You should be able to open this in FlashDevelop and compile/run it. The EventDataManager extends the Adobe implemented NetConnection class, which has two functions we are going to use, connect() and call(). We call connect with the address of the server (with the \u0026ldquo;/Amfphp/\u0026rdquo; directory tacked on) to establish a connection with the PHP service. The functions in the EventDataManager class are very simple pass-throughs to the call method. For example, this function calls the CreateEvent function in the EventService:\npublic function CreateEvent(name:String, location:String, cbCreateEvent:Responder=null):void{ call(\u0026quot;EventService/CreateEvent\u0026quot;, cbCreateEvent, name, location); } The Responder is created from a function which will be called after the response from the service is received. Take a look at Main.as to get an idea of how to use the data manager class. If you have any questions or notice any errors, please let me know.\nLinks/Reference  MySQL Command Quick Reference PHP MySQL Reference Amfphp Home Page  Files  Event Service Event Manager  ","permalink":"https://blachniet.github.io/posts/flash-data-storage-services-part-2/","summary":"In my last post we set up our EasyPHP server and created a MySQL database for our event manager. In this post we are going to create the Amfphp service to create, delete, and get events from the database. We are also going to create an ActionScript 3 class to talk to the PHP service.\nCreating the Service Your event service will be contained within a single PHP file in the Amfphp\\Services\\ directory.","title":"Flash Data Storage Services : Part 2"},{"content":"A core need of many online Flash applications is to store data in an online database. Supporting this need requires the integration of a few different technologies. The solution that I will be walking you through in this and the next tutorial post will integrate AS3, PHP, MySQL, and amfphp. This post will contain the boring, yet necessary, setup steps. This includes installing all tools and frameworks as well as creating the MySQL database to store your application data.\nTo demonstrate how to create a storage service for your Flash application, we are going to create a simple event management application. This Flash application will allow you to create, delete, and view events. All of the data for these events will be stored in a MySQL database via a PHP service. I do not own the Flash GUI, so we won\u0026rsquo;t be looking at any actual graphics for this application. We will just create the code to support the inexistent GUI.\nToolbox  FlashDevelop - This is the IDE that we will use to write all our code. It has built in support for ActionScript and PHP. I will be using version 4.0.0 Beta. EasyPHP - This is a development environment for a WAMP server (Windows, Apache, MySQL, PHP). As the name suggests, it is easy to use and requires almost no configuration. You get all the components installed at once, and there\u0026rsquo;s no configuration text files to mess with. This is by far the easiest way to get started with PHP development. amfphp - This library provides us with an easy way to create PHP services that ActionScript can communicate with using the Action Message Format.  Get the Server Up Install FlashDevelop and EasyPHP, and unzip Amfphp. After installing and running EasyPHP, right-click on the e on your task bar and select Explore.\n\nThis opens the top level directory that EasyPHP is serving. Copy your extracted Amfphp folder to this folder. You’re file structure should look something like this:\n\nTo make sure that everything is setup properly, navigate you favorite browser to http://127.0.0.1:8887/Amfphp/. You should see a page similar to this:\n\nMess around with the MirrorService to make sure that everything seems to be working properly.\nCreate the Database Now let’s get the database set up. Right click on the e for EasyPHP in your task bar and select Administration. On the Administration page, click the big green button that says Manage Your Databases. If your browser takes you to your phpMyAdmin page, then everything is fine and you can skip the next paragraph. If you got errors saying that you could not login, then it is probably because you have already installed MySQL and have set a password for the root user account. To fix this, open C:\\Program Files\\EasyPHP-5.3.6.1\\phpmyadmin\\config.inc.php (modify that path to match your setup if necessary). You will need to change the following 2 fields.\n$cfg['Servers'][$i]['password'] = 'yourPassword'; $cfg['Servers'][$i]['AllowNoPassword'] = false; Once you have successfully logged into phpMyAdmin, click the Databases tab, enter the name for your database, and click Create.\n\nClick on your newly created table, go to the SQL tab, enter the following SQL code, and click Go.\ncreate table events ( id bigint not null auto_increment primary key, name varchar(255) not null unique, location varchar(255) not null ); \nAll the boring setup is now complete. You have a working PHP server, a database to store your application data, and a framework to create services that can communicate with ActionScript. In Part 2 of this tutorial we will go over actually creating the PHP service that will manage this database as well as the AS3 that will communicate with the PHP service. If you have any trouble, or notice any typos, please let me know.\n","permalink":"https://blachniet.github.io/posts/flash-data-storage-services-part-1/","summary":"A core need of many online Flash applications is to store data in an online database. Supporting this need requires the integration of a few different technologies. The solution that I will be walking you through in this and the next tutorial post will integrate AS3, PHP, MySQL, and amfphp. This post will contain the boring, yet necessary, setup steps. This includes installing all tools and frameworks as well as creating the MySQL database to store your application data.","title":"Data Storage Services for Flash Applications : Part 1"},{"content":"A while back I wrote my first \u0026lsquo;Hello World\u0026rsquo; Android AIR application but never posted anything about it. Since I have this shiny new site, I figured I’d write up something real quick. If you know me at all, it would come as no surprise that I have not dropped the $700 or so on Flash CS5.5. That means that I did this application, a very simple Pong game, with only FlashDevelop.\nFilling Your Toolbox I\u0026rsquo;m not going to walk you through the creation of this game, I\u0026rsquo;m just going to get you set up to do your own app. To begin you are going to need to get the latest\nFlashDevelop (4.0.0 or later). Note that 4.0.0 and later are the only versions of FlashDevelop that support Android AIR development, and those releases are in Beta at the time of this posting. I never ran into any issues, but this is a heads up that it may not be fully tested yet. You will also have to grab the Android SDK if you don’t already have it.\nAndroids In Your FlashDevelop Open FlashDevelop, click Project \u0026raquo; New Project \u0026raquo; AS3 Android App to create your new project. FlashDevelop will generate a bunch of batch scripts and other files to get you started. Begin by reading the AIR_Android_readme.txt that FlashDevelop generates in your project. This file has a lot of useful instructions on how to get setup including how to generate the self-signed certificate required by AIR and running/debugging on your device. Once you\u0026rsquo;ve gone through the steps in that readme, you\u0026rsquo;re ready to start cranking out some ActionScript.\nCode Spelunking As I said, I’m not going to walk you through the making of my Pong game. This is because the majority of it is just normal AS3 code that is not specific to Android development (certainly not because I\u0026rsquo;m feeling lazy tonight). If you want, take a look at the code for my app (link at the bottom of this post) and grab the pieces you need to do your own application. You will probably need to regenerate the certificate by running /bat/CreateCertificate.bat. Let me know if you have any issues, or find some possible optimizations to my code (particularly in the TouchEvent department, it moves pretty slowly when there are multiple touches occurring at once). Booker has been doing some Android AIR development recently so keep an eye out for some sweetness over at his site. He’s actually got Flash CS5.5, so I expect he’ll have some hotter stuff and probably some more useful info. Download HelloAndroidAir.zip\n","permalink":"https://blachniet.github.io/posts/hello-android-air-world/","summary":"A while back I wrote my first \u0026lsquo;Hello World\u0026rsquo; Android AIR application but never posted anything about it. Since I have this shiny new site, I figured I’d write up something real quick. If you know me at all, it would come as no surprise that I have not dropped the $700 or so on Flash CS5.5. That means that I did this application, a very simple Pong game, with only FlashDevelop.","title":"Hello Android AIR World"},{"content":"In my recent exploration of web development in ASP.NET, I found what I assume to be a fairly common need to have part of a view/page update without the entire page updating. In my particular case, I wanted to have a page that listed items but also provided a form that allowed you to add an item. When an item was added, the list of items would be updated without having to regenerate the entire page. I eventually accomplished this through the unobtrusive AJAX provided in the MVC and jQuery frameworks. Below you can see the Razor page that was used to solve my problem. In this page, we include the jquery.unobtrusive-ajax.min.js in order to create a form using the AjaxHelper. The AJAX form indicates that it needs to call the action function Index_AddItem, and then update the productList when the form is submitted. At the bottom, in the productList div, we render the partial view, ProductListControl, with the list of products. (ProductIndexViewModel contains a Product and an IEnumerable. See Rachel Appel\u0026rsquo;s post for more information on ViewModels in MVC here: http://rachelappel.com/use-viewmodels-to-manage-data-amp-organize-code-in-asp.net-mvc-applications)\n@* Views\\Product\\Index.cshtml *@ @model BoringStore.ViewModels.ProductIndexViewModel @{ ViewBag.Title = \u0026quot;Index\u0026quot;; } \u0026lt;h2\u0026gt;Index\u0026lt;/h2\u0026gt; \u0026lt;script src=\u0026quot;@Url.Content(\u0026quot;~/Scripts/jquery.unobtrusive-ajax.min.js\u0026quot;)\u0026quot; type=\u0026quot;text/javascript\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; @using (Ajax.BeginForm(\u0026quot;Index_AddItem\u0026quot;, new AjaxOptions { UpdateTargetId = \u0026quot;productList\u0026quot; })) { \u0026lt;div\u0026gt; @Html.LabelFor(model =\u0026gt; model.NewProduct.Name) @Html.EditorFor(model =\u0026gt; model.NewProduct.Name) \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; @Html.LabelFor(model =\u0026gt; model.NewProduct.Price) @Html.EditorFor(model =\u0026gt; model.NewProduct.Price) \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;Add Product\u0026quot; /\u0026gt; \u0026lt;/div\u0026gt; } \u0026lt;div id='productList'\u0026gt; @{ Html.RenderPartial(\u0026quot;ProductListControl\u0026quot;, Model.Products); } \u0026lt;/div\u0026gt; The ProductListControl.cshtml is a very simple partial view that displays all the product names and prices in a table:\n@* Views\\Product\\ProductListControl.cshtml *@ @model IEnumerable\u0026lt;BoringStore.Models.Product\u0026gt; \u0026lt;table\u0026gt; \u0026lt;!-- Render the table headers. --\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Price\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;!-- Render the name and price of each product. --\u0026gt; @foreach (var item in Model) { \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;@Html.DisplayFor(model =\u0026gt; item.Name)\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;@Html.DisplayFor(model =\u0026gt; item.Price)\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; } \u0026lt;/table\u0026gt; The controller has a GET action for the Index and a POST action for adding the item. The Index_AddItem function is what the AJAX form calls when it is submitted. This function returns a partial view, allowing us to avoid regenerating the entire page.\n// Controllers\\ProductController.cs namespace BoringStore.Controllers { public class ProductController : Controller { public ActionResult Index() { BoringStoreContext db = new BoringStoreContext(); ProductIndexViewModel viewModel = new ProductIndexViewModel { NewProduct = new Product(), Products = db.Products }; return View(viewModel); } [HttpPost] public ActionResult Index_AddItem(ProductIndexViewModel viewModel) { BoringStoreContext db = new BoringStoreContext(); db.Products.Add(viewModel.NewProduct); db.SaveChanges(); return PartialView(\u0026quot;ProductListControl\u0026quot;, db.Products); } } } Summary: The unobtrusive AJAX provided by MVC and jQuery looks like it may be useful in many areas in my future web development. I hope this post helps get some people to this solution faster than I got to it. If you see any errors in the this post, or have suggestions for alternative ways of accomplishing the same goal, please feel free to share.\n","permalink":"https://blachniet.github.io/posts/partial-views-with-unobtrusive-ajax/","summary":"In my recent exploration of web development in ASP.NET, I found what I assume to be a fairly common need to have part of a view/page update without the entire page updating. In my particular case, I wanted to have a page that listed items but also provided a form that allowed you to add an item. When an item was added, the list of items would be updated without having to regenerate the entire page.","title":"Partial Views with Unobtrusive AJAX"},{"content":"As the title suggests, the goal of this site is not entirely ironed out. I\u0026rsquo;m actually trying to think of goals as I write this now, so lets start out with a simple introduction. I am a Computer Science graduate from Virginia Tech, class of 2009, with a minor in Mathematics. Since graduation I have worked on client-side gaming machines at Video Gaming Technologies, Inc. near Charlottesville, VA. While at VGT I have had the opportunity to expand my technical experience in both C# and C++, as well as get my feet wet in a few other languages including Python and ActionScript.\nNow that I have lost the interest of one of the two readers whose eyes will ever caress these words, here\u0026rsquo;s my current list of goals for this site:\n Provide myself with a platform for sharing/showing off some of my work. Provide readers with some helpful tips, tricks and tutorials in whatever technology I happen to be working in. Promote myself as a developer. Prove P = NP  That last one might be a little ambitious, but a nerd can dream. Hopefully I\u0026rsquo;ll be able to achieve most of these, but I\u0026rsquo;m counting on that one guy that\u0026rsquo;s still reading to let me know when I don\u0026rsquo;t.\nShout-outs\nA big thanks goes out to BH Booker and Swank Web Style for helping me get this site up and going. Definitely check out Swank Web Style for your web design and hosting needs.\n","permalink":"https://blachniet.github.io/posts/mission-less-statement/","summary":"As the title suggests, the goal of this site is not entirely ironed out. I\u0026rsquo;m actually trying to think of goals as I write this now, so lets start out with a simple introduction. I am a Computer Science graduate from Virginia Tech, class of 2009, with a minor in Mathematics. Since graduation I have worked on client-side gaming machines at Video Gaming Technologies, Inc. near Charlottesville, VA. While at VGT I have had the opportunity to expand my technical experience in both C# and C++, as well as get my feet wet in a few other languages including Python and ActionScript.","title":"Mission-less Statement"}]